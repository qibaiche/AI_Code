"""ä¸»å·¥ä½œæµæ§åˆ¶å™¨"""
import logging
import sys
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Optional
import pandas as pd

try:
    import win32gui
except ImportError:
    win32gui = None

from .config_loader import load_config, WorkflowConfig
from .data_reader import read_excel_file, save_result_excel, validate_data
from .mole_submitter import MoleSubmitter
from .spark_submitter import SparkSubmitter
from .gts_submitter import GTSSubmitter
from .mole_config_ui import show_mole_config_ui
from .utils.keyboard_listener import start_global_listener, is_esc_pressed, stop_global_listener

LOGGER = logging.getLogger(__name__)


class WorkflowError(Exception):
    """å·¥ä½œæµå¼‚å¸¸"""
    pass


class WorkflowController:
    """å·¥ä½œæµæ§åˆ¶å™¨"""
    
    def __init__(self, config: WorkflowConfig):
        self.config = config
        # åˆ›å»ºæœ¬æ¬¡è¿è¡Œçš„å·¥ä½œç›®å½•ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        self.work_dir = self._create_work_directory()
        # ä½¿ç”¨ MIR ç›®å½•ä½œä¸º debug ç›®å½•ï¼ˆç®€åŒ–ç»“æ„ï¼‰
        debug_dir = self.work_subdirs.get('mir', self.work_dir / '01_MIR')
        self.mole_submitter = MoleSubmitter(config.mole, debug_dir=debug_dir)
        self.spark_submitter = SparkSubmitter(config.spark, debug_dir=debug_dir)
        self.gts_submitter = GTSSubmitter(config.gts, debug_dir=debug_dir)
        self.results = []
        self.errors = []
        self.last_mir_result_file = None
        self.unit_comparison_details = []  # å­˜å‚¨è¯¦ç»†çš„unitå¯¹æ¯”ä¿¡æ¯
        self.available_units_export_file = None  # å­˜å‚¨ç¬¬ä¸€æ­¥éªŒè¯çš„å¯¼å‡ºæ–‡ä»¶è·¯å¾„
        self.units_validation_comparison_file = None  # å­˜å‚¨éªŒè¯æ¯”è¾ƒæ–‡ä»¶è·¯å¾„
        # åˆ›å»ºåˆå¹¶Excelæ–‡ä»¶è·¯å¾„ï¼ˆåœ¨ç”Ÿæˆæ—¶å°±å†™å…¥ï¼Œè€Œä¸æ˜¯æœ€ååˆå¹¶ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.merged_validation_file = self.work_subdirs['mir'] / f"Merged_Validation_Table_{timestamp}.xlsx"
        self.summary_table_file = None  # å­˜å‚¨æ±‡æ€»è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆåªåŒ…å« Source Lot, Part Type, Quantity, MIRï¼‰
        self.merged_excel_writer = None  # ç”¨äºä¿æŒExcelæ–‡ä»¶æ‰“å¼€çŠ¶æ€
        
        # å¯åŠ¨é”®ç›˜ç›‘å¬å™¨ï¼ˆESC é”®åœæ­¢ï¼‰
        def on_escape():
            """ESC é”®æŒ‰ä¸‹æ—¶çš„å¤„ç†"""
            LOGGER.warning("âš ï¸ æ£€æµ‹åˆ° ESC é”®ï¼Œæ­£åœ¨åœæ­¢ç¨‹åº...")
            self._cleanup_on_exit()
        
        start_global_listener(on_escape)
        LOGGER.info("ğŸ’¡ æç¤ºï¼šæŒ‰ ESC é”®å¯éšæ—¶åœæ­¢ç¨‹åº")
    
    def _cleanup_on_exit(self):
        """é€€å‡ºæ—¶çš„æ¸…ç†å·¥ä½œ"""
        try:
            # å…³é—­æµè§ˆå™¨
            if hasattr(self.spark_submitter, '_driver') and self.spark_submitter._driver:
                try:
                    self.spark_submitter._close_driver()
                except:
                    pass
            if hasattr(self.gts_submitter, 'driver') and self.gts_submitter.driver:
                try:
                    self.gts_submitter._close_browser()
                except:
                    pass
        except:
            pass
        finally:
            # çœŸæ­£é€€å‡ºç¨‹åº
            import sys
            LOGGER.warning("ç¨‹åºæ­£åœ¨é€€å‡º...")
            sys.exit(0)
    
    def _create_work_directory(self) -> Path:
        """
        åˆ›å»ºå·¥ä½œç›®å½•ï¼Œç›´æ¥åœ¨outputç›®å½•ä¸‹åˆ›å»ºä¸‰ä¸ªå­æ–‡ä»¶å¤¹
        
        Returns:
            å·¥ä½œç›®å½•è·¯å¾„ï¼ˆoutput_dirï¼‰
        """
        work_dir = self.config.paths.output_dir
        
        # ç›´æ¥åœ¨outputç›®å½•ä¸‹åˆ›å»ºå­ç›®å½•ï¼ˆ01_MIRã€02_SPARKã€03_GTSï¼‰
        subdirs = {
            'mir': work_dir / '01_MIR',
            'spark': work_dir / '02_SPARK',
            'gts': work_dir / '03_GTS'
        }
        
        for subdir in subdirs.values():
            subdir.mkdir(parents=True, exist_ok=True)
        
        self.work_subdirs = subdirs
        LOGGER.info(f"ğŸ“ å·¥ä½œç›®å½•: {work_dir}")
        LOGGER.info(f"   å­ç›®å½•ç»“æ„:")
        LOGGER.info(f"     - 01_MIR: {subdirs['mir']}")
        LOGGER.info(f"     - 02_SPARK: {subdirs['spark']}")
        LOGGER.info(f"     - 03_GTS: {subdirs['gts']}")
        
        return work_dir
    
    def run_workflow(self, excel_file_path: str | Path) -> Path | None:
        """
        è¿è¡Œå®Œæ•´çš„å·¥ä½œæµ
        
        Args:
            excel_file_path: Excelæ–‡ä»¶è·¯å¾„
        
        Returns:
            è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„
        
        Raises:
            WorkflowError: å¦‚æœå·¥ä½œæµæ‰§è¡Œå¤±è´¥
        """
        excel_file_path = Path(excel_file_path)
        
        LOGGER.info("=" * 80)
        LOGGER.info("å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–å·¥ä½œæµ")
        LOGGER.info(f"è¾“å…¥æ–‡ä»¶: {excel_file_path}")
        LOGGER.info("=" * 80)
        
        start_time = datetime.now()
        
        try:
            # æ£€æŸ¥ ESC é”®
            if is_esc_pressed():
                LOGGER.warning("âš ï¸ ç¨‹åºå·²åœæ­¢ï¼ˆESC é”®ï¼‰")
                return None
            
            # æ­¥éª¤0: é¢„å…ˆå¯åŠ¨Moleå·¥å…·
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ 0/5: å¯åŠ¨Moleå·¥å…·")
            LOGGER.info("=" * 80)
            self._step_start_mole()
            
            # æ£€æŸ¥ ESC é”®
            if is_esc_pressed():
                LOGGER.warning("âš ï¸ ç¨‹åºå·²åœæ­¢ï¼ˆESC é”®ï¼‰")
                return None
            
            # æ­¥éª¤1: è¯»å–æ–‡ä»¶ï¼ˆExcelæˆ–CSVï¼‰
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ 1/5: è¯»å–source lotæ–‡ä»¶")
            LOGGER.info("=" * 80)
            df = self._step_read_excel(excel_file_path)
            
            # æ£€æŸ¥ ESC é”®
            if is_esc_pressed():
                LOGGER.warning("âš ï¸ ç¨‹åºå·²åœæ­¢ï¼ˆESC é”®ï¼‰")
                return None
            
            # æ­¥éª¤2: æäº¤MIRæ•°æ®åˆ°Moleå·¥å…·ï¼ˆå¾ªç¯å¤„ç†æ‰€æœ‰è¡Œï¼‰
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ 2/5: æäº¤MIRæ•°æ®åˆ°Moleå·¥å…·ï¼ˆå¾ªç¯å¤„ç†æ‰€æœ‰è¡Œï¼‰")
            LOGGER.info("=" * 80)
            # æ£€æŸ¥ ESC é”®
            if is_esc_pressed():
                LOGGER.warning("âš ï¸ ç¨‹åºå·²åœæ­¢ï¼ˆESC é”®ï¼‰")
                return None
            # è·å–Source Lotæ–‡ä»¶è·¯å¾„
            source_lot_file_path = self._get_source_lot_file_path(excel_file_path)
            self._step_submit_to_mole(df, source_lot_file_path)
            
            # æ£€æŸ¥ ESC é”®
            if is_esc_pressed():
                LOGGER.warning("âš ï¸ ç¨‹åºå·²åœæ­¢ï¼ˆESC é”®ï¼‰")
                return None
            
            # æ­¥éª¤3: æäº¤VPOæ•°æ®åˆ°Sparkç½‘é¡µ
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ 3/5: æäº¤VPOæ•°æ®åˆ°Sparkç½‘é¡µ")
            LOGGER.info("=" * 80)
            self._step_submit_to_spark(df)
            
            # æ£€æŸ¥ ESC é”®
            if is_esc_pressed():
                LOGGER.warning("âš ï¸ ç¨‹åºå·²åœæ­¢ï¼ˆESC é”®ï¼‰")
                return None
            
            # æ­¥éª¤4: ç”ŸæˆGTSå¡«å……æ–‡ä»¶
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ 4/5: ç”ŸæˆGTSå¡«å……æ–‡ä»¶")
            LOGGER.info("=" * 80)
            self._step_generate_gts_file()
            
            # æ£€æŸ¥ ESC é”®
            if is_esc_pressed():
                LOGGER.warning("âš ï¸ ç¨‹åºå·²åœæ­¢ï¼ˆESC é”®ï¼‰")
                return None
            
            # æ­¥éª¤5: è‡ªåŠ¨å¡«å……å¹¶æäº¤GTS
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ 5/5: è‡ªåŠ¨å¡«å……å¹¶æäº¤GTS")
            LOGGER.info("=" * 80)
            self._step_submit_to_gts()
            
            # ä¿å­˜ç»“æœ
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("ä¿å­˜å¤„ç†ç»“æœ")
            LOGGER.info("=" * 80)
            output_path = self._step_save_results(df)
            
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            elapsed_time = (datetime.now() - start_time).total_seconds()
            
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("âœ… å®Œæ•´å·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼")
            LOGGER.info("=" * 80)
            LOGGER.info("å·²å®Œæˆæ‰€æœ‰æ­¥éª¤:")
            LOGGER.info("  âœ“ Mole: MIR æ•°æ®å·²æäº¤")
            LOGGER.info("  âœ“ Spark: VPO æ•°æ®å·²æäº¤")
            LOGGER.info("  âœ“ GTS: å¡«å……æ–‡ä»¶å·²ç”Ÿæˆå¹¶æäº¤")
            LOGGER.info(f"æ‰§è¡Œæ—¶é—´: {elapsed_time:.2f} ç§’")
            if output_path:
                LOGGER.info(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
            else:
                LOGGER.info("æ³¨æ„: MIRç»“æœå·²ä¿å­˜ä¸ºCSVæ–‡ä»¶")
            LOGGER.info(f"ğŸ“ æœ¬æ¬¡è¿è¡Œçš„æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {self.work_dir}")
            LOGGER.info("   æ–‡ä»¶åˆ†ç±»:")
            LOGGER.info(f"     - MIRç»“æœ: {self.work_subdirs['mir'].name}")
            LOGGER.info(f"     - Sparkæ–‡ä»¶: {self.work_subdirs['spark'].name}")
            LOGGER.info(f"     - GTSæ–‡ä»¶: {self.work_subdirs['gts'].name}")
            LOGGER.info("=" * 80)
            
            return output_path
            
        except Exception as e:
            elapsed_time = (datetime.now() - start_time).total_seconds()
            error_msg = f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}"
            LOGGER.error("\n" + "=" * 80)
            LOGGER.error(f"âŒ {error_msg}")
            LOGGER.error(f"æ‰§è¡Œæ—¶é—´: {elapsed_time:.2f} ç§’")
            LOGGER.error("=" * 80)
            LOGGER.error(traceback.format_exc())
            raise WorkflowError(error_msg) from e
    
    def run_mole_only(self, excel_file_path: str | Path) -> Path | None:
        """
        ä»…è¿è¡ŒMoleæ­¥éª¤ï¼ˆä¸æ‰§è¡ŒSpark/GTSï¼‰
        
        Args:
            excel_file_path: source lotæ–‡ä»¶è·¯å¾„
        
        Returns:
            æœ€æ–°çš„MIRç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœç”Ÿæˆï¼‰
        """
        excel_file_path = Path(excel_file_path)
        
        LOGGER.info("=" * 80)
        LOGGER.info("å¼€å§‹æ‰§è¡ŒMole-onlyå·¥ä½œæµï¼ˆSpark/GTSå·²è·³è¿‡ï¼‰")
        LOGGER.info(f"è¾“å…¥æ–‡ä»¶: {excel_file_path}")
        LOGGER.info("=" * 80)
        
        start_time = datetime.now()
        
        try:
            # æ­¥éª¤-1: æ˜¾ç¤ºé…ç½®UI
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ -1/3: é…ç½®Moleå‚æ•°")
            LOGGER.info("=" * 80)
            
            # è·å–config.yamlè·¯å¾„
            config_path = Path(__file__).parent / "config.yaml"
            
            # æ˜¾ç¤ºé…ç½®UI
            LOGGER.info("æ˜¾ç¤ºMoleé…ç½®ç•Œé¢...")
            ui_config = show_mole_config_ui(config_path)
            
            if ui_config is None:
                LOGGER.warning("ç”¨æˆ·å–æ¶ˆäº†é…ç½®ï¼Œå·¥ä½œæµç»ˆæ­¢")
                return None
            
            LOGGER.info(f"ç”¨æˆ·é…ç½®: {ui_config}")
            
            # æ›´æ–°mole_submitterçš„é…ç½®
            self.mole_submitter.config.search_mode = ui_config.get('search_mode', 'vpos')
            self.mole_submitter.config.ui_config = ui_config
            
            # ä¿å­˜UIé…ç½®åˆ°å®ä¾‹å˜é‡ï¼Œä¾›Sparkæäº¤æ—¶ä½¿ç”¨
            self.ui_config = ui_config
            
            # æ­¥éª¤0: é¢„å…ˆå¯åŠ¨Moleå·¥å…·
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ 0/3: å¯åŠ¨Moleå·¥å…·")
            LOGGER.info("=" * 80)
            self._step_start_mole()
            
            # æ­¥éª¤1: è¯»å–æ–‡ä»¶ï¼ˆExcelæˆ–CSVï¼‰
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ 1/3: è¯»å–source lotæ–‡ä»¶")
            LOGGER.info("=" * 80)
            df = self._step_read_excel(excel_file_path)
            
            # æ­¥éª¤2: æäº¤MIRæ•°æ®åˆ°Moleå·¥å…·ï¼ˆå¾ªç¯å¤„ç†æ‰€æœ‰è¡Œï¼‰
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("æ­¥éª¤ 2/3: æäº¤MIRæ•°æ®åˆ°Moleå·¥å…·ï¼ˆå¾ªç¯å¤„ç†æ‰€æœ‰è¡Œï¼‰")
            LOGGER.info("=" * 80)
            source_lot_file_path = self._get_source_lot_file_path(excel_file_path)
            self._step_submit_to_mole(df, source_lot_file_path, ui_config)
            
            # æ­¥éª¤2.5: ç”Ÿæˆåˆå¹¶æ–‡ä»¶ï¼ˆMIRç»“æœ + For Spark.csvï¼‰
            if self.last_mir_result_file and self.last_mir_result_file.exists():
                LOGGER.info("\n" + "=" * 80)
                LOGGER.info("æ­¥éª¤ 2.5/3: ç”Ÿæˆåˆå¹¶æ–‡ä»¶ï¼ˆMIRç»“æœ + For Spark.csvï¼‰")
                LOGGER.info("=" * 80)
                try:
                    # è¯»å–MIRç»“æœæ–‡ä»¶
                    mir_df = read_excel_file(self.last_mir_result_file)
                    
                    # æŸ¥æ‰¾ For Spark.csv æ–‡ä»¶
                    base_dir = Path(__file__).parent.parent
                    possible_spark_config_paths = [
                        base_dir / "input" / "For Spark.csv",
                        base_dir / "For Spark.csv",
                        self.config.paths.input_dir / "For Spark.csv"
                    ]
                    
                    spark_config_file = None
                    for path in possible_spark_config_paths:
                        if path.exists():
                            spark_config_file = path
                            break
                    
                    # åˆå¹¶æ–‡ä»¶
                    merged_df = self._merge_mir_with_spark_config(mir_df, spark_config_file)
                    
                    # ä¿å­˜åˆå¹¶æ–‡ä»¶åˆ° output æ ¹ç›®å½•
                    date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
                    merged_file = self.work_subdirs['spark'] / f"MIR_Results_For_Spark_{date_str}.xlsx"
                    try:
                        merged_df.to_excel(merged_file, index=False, engine='openpyxl')
                        LOGGER.info(f"âœ… å·²ç”Ÿæˆåˆå¹¶æ–‡ä»¶: {merged_file}")
                        LOGGER.info(f"   åŒ…å« {len(merged_df)} è¡Œæ•°æ®")
                        LOGGER.info(f"   åˆ—: {merged_df.columns.tolist()}")
                    except Exception as e:
                        # å¦‚æœExcelä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜ä¸ºCSV
                        LOGGER.warning(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•ä¿å­˜ä¸ºCSVæ ¼å¼...")
                        merged_file = self.work_subdirs['spark'] / f"MIR_Results_For_Spark_{date_str}.csv"
                        merged_df.to_csv(merged_file, index=False, encoding='utf-8-sig')
                        LOGGER.info(f"âœ… å·²ç”Ÿæˆåˆå¹¶æ–‡ä»¶: {merged_file} (CSVæ ¼å¼)")
                except Exception as e:
                    LOGGER.warning(f"ç”Ÿæˆåˆå¹¶æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œç»§ç»­æ‰§è¡Œ...")
                    LOGGER.debug(traceback.format_exc())
            
            elapsed_time = (datetime.now() - start_time).total_seconds()
            output_path = self.last_mir_result_file
            
            LOGGER.info("\n" + "=" * 80)
            LOGGER.info("âœ… Mole-onlyå·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼ˆå·²è·³è¿‡Spark/GTSï¼‰")
            LOGGER.info(f"æ‰§è¡Œæ—¶é—´: {elapsed_time:.2f} ç§’")
            if output_path:
                LOGGER.info(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
            else:
                LOGGER.info("æ³¨æ„: æœªè·å–åˆ°MIRç»“æœæ–‡ä»¶è·¯å¾„")
            
            # æŸ¥æ‰¾å¹¶æ˜¾ç¤ºåˆå¹¶æ–‡ä»¶
            merged_files = sorted(self.work_subdirs['spark'].glob("MIR_Results_For_Spark_*.xlsx"), reverse=True)
            merged_files.extend(sorted(self.work_subdirs['spark'].glob("MIR_Results_For_Spark_*.csv"), reverse=True))
            if merged_files:
                latest_merged = merged_files[0]
                LOGGER.info(f"ğŸ“Š åˆå¹¶æ–‡ä»¶ï¼ˆMIR + For Spark.csvï¼‰: {latest_merged}")
                LOGGER.info(f"   ä½ç½®: {latest_merged.parent}")
            
            LOGGER.info(f"ğŸ“ æœ¬æ¬¡è¿è¡Œçš„æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {self.work_dir}")
            LOGGER.info("=" * 80)
            
            return output_path
            
        except Exception as e:
            elapsed_time = (datetime.now() - start_time).total_seconds()
            error_msg = f"Mole-onlyå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}"
            LOGGER.error("\n" + "=" * 80)
            LOGGER.error(f"âŒ {error_msg}")
            LOGGER.error(f"æ‰§è¡Œæ—¶é—´: {elapsed_time:.2f} ç§’")
            LOGGER.error("=" * 80)
            LOGGER.error(traceback.format_exc())
            raise WorkflowError(error_msg) from e
    
    def _get_source_lot_file_path(self, excel_file_path: Path) -> Path:
        """è·å–Source Lotæ–‡ä»¶è·¯å¾„"""
        # ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
        if self.config.paths.source_lot_file and self.config.paths.source_lot_file.exists():
            return self.config.paths.source_lot_file
        
        # çˆ¶ç›®å½•ï¼ˆAuto VPOæ ¹ç›®å½•ï¼‰
        parent_dir = excel_file_path.parent
        
        # å¯èƒ½çš„æ–‡ä»¶ååˆ—è¡¨
        possible_names = [
            "Source Lot.csv",
            "Source Lot.xlsx",
            "Source Lot.xls",
            "source lot.csv",
            "source lot.xlsx",
            "source lot.xls",
        ]
        
        # ä¼˜å…ˆåœ¨ input/ ç›®å½•ä¸‹æŸ¥æ‰¾
        input_dir = parent_dir / "input"
        if input_dir.exists():
            for filename in possible_names:
                file_path = input_dir / filename
                if file_path.exists():
                    LOGGER.info(f"åœ¨inputç›®å½•æ‰¾åˆ°Source Lotæ–‡ä»¶: {file_path}")
                    return file_path
        
        # åœ¨çˆ¶ç›®å½•ï¼ˆAuto VPOæ ¹ç›®å½•ï¼‰ä¸­æŸ¥æ‰¾
        for filename in possible_names:
            file_path = parent_dir / filename
            if file_path.exists():
                LOGGER.info(f"åœ¨æ ¹ç›®å½•æ‰¾åˆ°Source Lotæ–‡ä»¶: {file_path}")
                return file_path
        
        raise WorkflowError(f"æœªæ‰¾åˆ°Source Lotæ–‡ä»¶ã€‚è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨äºä»¥ä¸‹ä½ç½®ä¹‹ä¸€:\n  - {input_dir if input_dir.exists() else parent_dir / 'input'}\n  - {parent_dir}")
    
    def _save_all_mir_results(self, source_lot_file_path: Path, mir_results: list) -> None:
        """
        ä¿å­˜æ‰€æœ‰MIRç»“æœåˆ°CSVæ–‡ä»¶
        
        Args:
            source_lot_file_path: Source Lotæ–‡ä»¶è·¯å¾„
            mir_results: MIRç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«åŸå§‹è¡Œæ•°æ®+MIRåˆ—
        """
        try:
            if not mir_results:
                LOGGER.warning("æ²¡æœ‰MIRç»“æœéœ€è¦ä¿å­˜")
                return
            
            # åˆ›å»ºDataFrame
            result_df = pd.DataFrame(mir_results)
            
            # ç»Ÿä¸€åˆ—åï¼šå°† Source æˆ– SourceLot é‡å‘½åä¸º Source Lot
            column_mapping = {}
            for col in result_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['SOURCE', 'SOURCELOT', 'SOURCE_LOT', 'SOURCELOTS', 'SOURCE LOTS'] and col != 'Source Lot':
                    column_mapping[col] = 'Source Lot'
                    LOGGER.info(f"å°†åˆ— '{col}' é‡å‘½åä¸º 'Source Lot'")
            
            if column_mapping:
                result_df = result_df.rename(columns=column_mapping)
            
            # æŸ¥æ‰¾SourceLotåˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            source_lot_col = None
            for col in result_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT', 'SOURCELOTS', 'SOURCE LOTS', 'SOURCE']:
                    source_lot_col = col
                    break
            
            # å¦‚æœæ‰¾åˆ°äº† Source Lot åˆ—ï¼Œç¡®ä¿åˆ—åä¸º "Source Lot"
            if source_lot_col and source_lot_col != 'Source Lot':
                result_df = result_df.rename(columns={source_lot_col: 'Source Lot'})
                source_lot_col = 'Source Lot'
                LOGGER.info(f"å·²ç»Ÿä¸€åˆ—åä¸º 'Source Lot'")
            
            # å¦‚æœæ‰¾åˆ°SourceLotåˆ—ï¼ŒæŒ‰SourceLotåˆ†ç»„ï¼Œç¡®ä¿ç›¸åŒSourceLotçš„è¡Œæ”¾åœ¨ä¸€èµ·
            # åŒæ—¶ä¿æŒsource lotè¡¨çš„åŸå§‹é¡ºåºï¼ˆç›¸åŒSourceLoté¦–æ¬¡å‡ºç°çš„é¡ºåºï¼‰
            if source_lot_col:
                LOGGER.info(f"æŒ‰SourceLotåˆ— '{source_lot_col}' åˆ†ç»„ï¼Œç¡®ä¿ç›¸åŒSourceLotçš„MIRæ”¾åœ¨ä¸€èµ·...")
                
                # æ·»åŠ åŸå§‹ç´¢å¼•åˆ—ï¼Œç”¨äºä¿æŒåŸå§‹é¡ºåº
                result_df['_original_index'] = range(len(result_df))
                
                # è®°å½•æ¯ä¸ªSourceLoté¦–æ¬¡å‡ºç°çš„ç´¢å¼•
                first_occurrence = {}
                for idx, source_lot_value in enumerate(result_df[source_lot_col]):
                    if pd.notna(source_lot_value):
                        source_lot_str = str(source_lot_value).strip()
                        if source_lot_str and source_lot_str not in first_occurrence:
                            first_occurrence[source_lot_str] = idx
                
                # åˆ›å»ºåˆ†ç»„é”®ï¼šSourceLoté¦–æ¬¡å‡ºç°çš„ç´¢å¼• + SourceLotå€¼
                def get_group_key(row):
                    source_lot_value = row[source_lot_col]
                    if pd.isna(source_lot_value):
                        return (float('inf'), '')  # NaNå€¼æ”¾åœ¨æœ€å
                    source_lot_str = str(source_lot_value).strip()
                    first_idx = first_occurrence.get(source_lot_str, float('inf'))
                    return (first_idx, source_lot_str)
                
                # åˆ›å»ºåˆ†ç»„é”®åˆ—
                result_df['_group_key'] = result_df.apply(get_group_key, axis=1)
                
                # æŒ‰åˆ†ç»„é”®å’ŒåŸå§‹ç´¢å¼•æ’åº
                result_df = result_df.sort_values(by=['_group_key', '_original_index'], na_position='last')
                
                # åˆ é™¤ä¸´æ—¶åˆ—
                result_df = result_df.drop(columns=['_original_index', '_group_key'])
                
                LOGGER.info(f"âœ… å·²æŒ‰SourceLotåˆ†ç»„ï¼Œç›¸åŒSourceLotçš„MIRå·²æ”¾åœ¨ä¸€èµ·ï¼ˆä¿æŒsource lotè¡¨çš„åŸå§‹é¡ºåºï¼‰")
            else:
                LOGGER.warning("æœªæ‰¾åˆ°SourceLotåˆ—ï¼Œä¿æŒåŸå§‹é¡ºåº")
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆä½¿ç”¨å·¥ä½œç›®å½•ï¼‰
            date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = self.work_subdirs['mir'] / f"MIR_Results_{date_str}.xlsx"
            
            # ä¿å­˜åˆ°Excelï¼ˆå¤šä¸ªå·¥ä½œè¡¨ï¼‰
            try:
                from openpyxl import Workbook
                from openpyxl.utils.dataframe import dataframe_to_rows
                
                wb = Workbook()
                wb.remove(wb.active)  # åˆ é™¤é»˜è®¤sheet
                
                # Sheet 1: æŒ‰Source Lotçš„æ€»ç»“ä¿¡æ¯
                summary_sheet = wb.create_sheet("Summary by Source Lot")
                for r in dataframe_to_rows(result_df, index=False, header=True):
                    summary_sheet.append(r)
                
                # Sheet 2: è¯¦ç»†çš„Unit Nameå¯¹æ¯”ï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(self, 'unit_comparison_details') and self.unit_comparison_details:
                    unit_comparison_df = pd.DataFrame(self.unit_comparison_details)
                    
                    # æŒ‰Source Lotå’ŒStatusæ’åº
                    if 'Source Lot' in unit_comparison_df.columns:
                        unit_comparison_df = unit_comparison_df.sort_values(
                            by=['Source Lot', 'Status', 'Unit Name'],
                            ascending=[True, True, True]
                        )
                    
                    detail_sheet = wb.create_sheet("Unit Comparison Details")
                    for r in dataframe_to_rows(unit_comparison_df, index=False, header=True):
                        detail_sheet.append(r)
                    
                    # Sheet 3: ä¸åŒ¹é…çš„Unitsï¼ˆMissingå’ŒExtraï¼‰
                    unmatched_df = unit_comparison_df[unit_comparison_df['Status'].isin(['Missing', 'Extra'])].copy()
                    if not unmatched_df.empty:
                        unmatched_sheet = wb.create_sheet("Unmatched Units")
                        for r in dataframe_to_rows(unmatched_df, index=False, header=True):
                            unmatched_sheet.append(r)
                        LOGGER.info(f"   ä¸åŒ¹é…çš„Units: {len(unmatched_df)} ä¸ª")
                    else:
                        LOGGER.info(f"   âœ… æ‰€æœ‰Unitséƒ½åŒ¹é…")
                    
                    LOGGER.info(f"   è¯¦ç»†çš„Unitå¯¹æ¯”: {len(unit_comparison_df)} è¡Œ")
                
                wb.save(output_file)
                LOGGER.info(f"âœ… æ‰€æœ‰MIRç»“æœå·²ä¿å­˜åˆ°: {output_file}")
                LOGGER.info(f"   å…± {len(mir_results)} ä¸ªSource Lotçš„æ€»ç»“ä¿¡æ¯")
                if hasattr(self, 'unit_comparison_details') and self.unit_comparison_details:
                    total_units = len(self.unit_comparison_details)
                    matched_count = len([u for u in self.unit_comparison_details if u.get('Status') == 'Matched'])
                    missing_count = len([u for u in self.unit_comparison_details if u.get('Status') == 'Missing'])
                    extra_count = len([u for u in self.unit_comparison_details if u.get('Status') == 'Extra'])
                    LOGGER.info(f"   è¯¦ç»†çš„Unitå¯¹æ¯”: æ€»è®¡ {total_units} ä¸ªunits")
                    LOGGER.info(f"     - åŒ¹é…: {matched_count} ä¸ª")
                    LOGGER.info(f"     - ç¼ºå¤±: {missing_count} ä¸ª")
                    LOGGER.info(f"     - é¢å¤–: {extra_count} ä¸ª")
                LOGGER.info(f"   æ–‡ä»¶æ ¼å¼: Excel (.xlsx)")
                LOGGER.info(f"   åŒ…å«å·¥ä½œè¡¨: Summary by Source Lot" + 
                           (", Unit Comparison Details, Unmatched Units" if hasattr(self, 'unit_comparison_details') and self.unit_comparison_details else ""))
            except Exception as e:
                # å¦‚æœExcelä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜ä¸ºCSV
                LOGGER.warning(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•ä¿å­˜ä¸ºCSVæ ¼å¼...")
                csv_file = self.work_subdirs['mir'] / f"MIR_Results_{date_str}.csv"
                result_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                output_file = csv_file
                LOGGER.info(f"âœ… æ‰€æœ‰MIRç»“æœå·²ä¿å­˜åˆ°: {output_file} (CSVæ ¼å¼)")
            
            self.last_mir_result_file = output_file
            
            # æ˜¾ç¤ºæ¯è¡Œçš„è¯¦ç»†ä¿¡æ¯ï¼ˆæŒ‰æ’åºåçš„é¡ºåºï¼‰
            for idx, (_, row) in enumerate(result_df.iterrows(), 1):
                source_lot = row.get(source_lot_col, 'N/A') if source_lot_col else 'N/A'
                mir = row.get('MIR', 'N/A')
                LOGGER.info(f"   ç¬¬ {idx} è¡Œ: SourceLot={source_lot}, MIR={mir}")
            
            return output_file
            
        except Exception as e:
            LOGGER.error(f"ä¿å­˜MIRç»“æœåˆ°CSVå¤±è´¥: {e}")
            import traceback
            LOGGER.error(traceback.format_exc())
    
    def _generate_summary_table_for_vpos(self, source_lot_file_path: Path, mir_results: list) -> None:
        """
        ä¸º VPOs æ¨¡å¼ç”Ÿæˆæ±‡æ€»è¡¨ï¼ˆåªåŒ…å« Source Lot, Part Type, Quantity, MIRï¼‰
        
        Args:
            source_lot_file_path: Source Lotæ–‡ä»¶è·¯å¾„
            mir_results: MIRç»“æœåˆ—è¡¨
        """
        try:
            # è¯»å– Source Lot æ–‡ä»¶è·å– Part Type å’Œ Quantity
            if not source_lot_file_path or not source_lot_file_path.exists():
                LOGGER.warning(f"Source Lot æ–‡ä»¶ä¸å­˜åœ¨: {source_lot_file_path}ï¼Œæ— æ³•ç”Ÿæˆæ±‡æ€»è¡¨")
                return
            
            LOGGER.info(f"è¯»å– Source Lot æ–‡ä»¶: {source_lot_file_path}")
            source_lot_df = read_excel_file(source_lot_file_path)
            LOGGER.info(f"  - åŒ…å« {len(source_lot_df)} è¡Œæ•°æ®")
            
            # æŸ¥æ‰¾åˆ—å
            source_lot_col = None
            for col in source_lot_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT', 'SOURCELOTS', 'SOURCE LOTS', 'SOURCE']:
                    source_lot_col = col
                    LOGGER.info(f"  æ‰¾åˆ° Source Lot åˆ—: '{col}'")
                    break
            
            part_type_col = None
            for col in source_lot_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['PART TYPE', 'PARTTYPE', 'PART_TYPE']:
                    part_type_col = col
                    LOGGER.info(f"  æ‰¾åˆ° Part Type åˆ—: '{col}'")
                    break
            
            quantity_col = None
            for col in source_lot_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['QUANTITY', 'QTY', 'QTY.', 'COUNT']:
                    quantity_col = col
                    LOGGER.info(f"  æ‰¾åˆ° Quantity åˆ—: '{col}'")
                    break
            
            if not source_lot_col:
                LOGGER.warning(f"æœªæ‰¾åˆ° Source Lot åˆ—ï¼Œæ— æ³•ç”Ÿæˆæ±‡æ€»è¡¨ã€‚å¯ç”¨åˆ—: {source_lot_df.columns.tolist()}")
                return
            
            # åˆ›å»º MIR æ˜ å°„ï¼ˆsource lot -> MIRï¼‰
            mir_map = {}
            for result in mir_results:
                source_lot = result.get('Source Lot', '') or result.get(source_lot_col, '')
                mir = result.get('MIR', '')
                if source_lot and mir:
                    mir_map[str(source_lot).strip()] = str(mir).strip()
            
            # åˆ›å»ºæ±‡æ€»è¡¨
            summary_list = []
            for _, row in source_lot_df.iterrows():
                source_lot_value = str(row[source_lot_col]).strip() if pd.notna(row[source_lot_col]) else ''
                if not source_lot_value:
                    continue
                
                part_type = ''
                if part_type_col and pd.notna(row.get(part_type_col)):
                    part_type = str(row[part_type_col]).strip()
                
                quantity = ''
                if quantity_col and pd.notna(row.get(quantity_col)):
                    quantity = str(row[quantity_col]).strip()
                
                mir = mir_map.get(source_lot_value, '')
                
                summary_list.append({
                    'Source Lot': source_lot_value,
                    'Part Type': part_type,
                    'Quantity': quantity,
                    'MIR': mir
                })
            
            if not summary_list:
                LOGGER.warning("æ²¡æœ‰æ•°æ®å¯ç”Ÿæˆæ±‡æ€»è¡¨")
                return
            
            summary_df = pd.DataFrame(summary_list)
            summary_df = summary_df.sort_values(by='Source Lot', ascending=True)
            
            # ä¿å­˜æ±‡æ€»è¡¨
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_output_file = self.work_subdirs['mir'] / f"Summary_Table_{timestamp}.xlsx"
            self.summary_table_file = summary_output_file
            
            summary_df.to_excel(summary_output_file, index=False, engine='openpyxl')
            LOGGER.info(f"âœ… VPOsæ¨¡å¼æ±‡æ€»è¡¨å·²ä¿å­˜åˆ°: {summary_output_file}")
            LOGGER.info(f"   åŒ…å«åˆ—: Source Lot, Part Type, Quantity, MIR ({len(summary_df)} è¡Œ)")
            
            # åœ¨å†™å…¥ MIR åï¼Œå°† Summary_Table é‡å‘½åä¸º Spark ä¼šè°ƒç”¨çš„è¡¨çš„åå­—
            # é‡å‘½åä¸º MIR_Results_*.xlsxï¼ˆä½¿ç”¨å½“å‰æ—¶é—´æˆ³ï¼‰ï¼Œè¿™æ · Spark æ­¥éª¤å¯ä»¥è¯»å–å®ƒ
            date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            spark_summary_file = self.work_subdirs['mir'] / f"MIR_Results_{date_str}.xlsx"
            
            try:
                # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if spark_summary_file.exists():
                    spark_summary_file.unlink()
                
                # é‡å‘½åæ–‡ä»¶
                summary_output_file.rename(spark_summary_file)
                self.summary_table_file = spark_summary_file
                LOGGER.info(f"âœ… æ±‡æ€»è¡¨å·²é‡å‘½åä¸º: {spark_summary_file.name}ï¼ˆä¾› Spark ä½¿ç”¨ï¼‰")
            except Exception as e:
                LOGGER.warning(f"é‡å‘½åæ±‡æ€»è¡¨å¤±è´¥: {e}ï¼Œä¿æŒåŸæ–‡ä»¶å: {summary_output_file.name}")
            
        except Exception as e:
            LOGGER.error(f"ç”Ÿæˆ VPOs æ¨¡å¼æ±‡æ€»è¡¨å¤±è´¥: {e}")
            import traceback
            LOGGER.error(traceback.format_exc())
    
    def _merge_validation_tables_immediately(self, source_lot_file_path: Path | None, units_df: pd.DataFrame, source_lot_col: str, unit_name_col: str) -> None:
        """
        åœ¨ç”Ÿæˆä¸¤å¼ è¡¨æ—¶ç«‹å³åˆå¹¶ available_units_export å’Œ Units_Validation_Comparison è¡¨
        ç”ŸæˆåŒ…å« source lot, Part Type, quantity çš„åˆå¹¶è¡¨ï¼ˆæ­¤æ—¶MIRè¿˜æ²¡æœ‰ï¼‰
        
        Args:
            source_lot_file_path: Source Lotæ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥ä¸ºNoneï¼Œä¼šä»units_dfä¸­è·å–ä¿¡æ¯ï¼‰
            units_df: Units DataFrameï¼ˆåŒ…å« source lot å’Œ unit name ä¿¡æ¯ï¼‰
            source_lot_col: Source Lot åˆ—å
            unit_name_col: Unit Name åˆ—å
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not self.available_units_export_file or not self.available_units_export_file.exists():
                LOGGER.warning(f"available_units_export æ–‡ä»¶ä¸å­˜åœ¨: {self.available_units_export_file}")
                return
            
            if not self.units_validation_comparison_file or not self.units_validation_comparison_file.exists():
                LOGGER.warning(f"Units_Validation_Comparison æ–‡ä»¶ä¸å­˜åœ¨: {self.units_validation_comparison_file}")
                return
            
            LOGGER.info(f"è¯»å– available_units_export æ–‡ä»¶: {self.available_units_export_file}")
            available_units_df = read_excel_file(self.available_units_export_file)
            LOGGER.info(f"  - åŒ…å« {len(available_units_df)} è¡Œæ•°æ®")
            
            LOGGER.info(f"è¯»å– Units_Validation_Comparison æ–‡ä»¶: {self.units_validation_comparison_file}")
            # å¦‚æœæ˜¯Excelæ–‡ä»¶ï¼Œè¯»å– 'All Units Comparison' sheet
            if self.units_validation_comparison_file.suffix.lower() == '.xlsx':
                try:
                    validation_df = pd.read_excel(self.units_validation_comparison_file, sheet_name='All Units Comparison')
                except:
                    # å¦‚æœæ²¡æœ‰è¯¥sheetï¼Œè¯»å–ç¬¬ä¸€ä¸ªsheet
                    validation_df = read_excel_file(self.units_validation_comparison_file)
            else:
                validation_df = read_excel_file(self.units_validation_comparison_file)
            LOGGER.info(f"  - åŒ…å« {len(validation_df)} è¡Œæ•°æ®")
            
            # è¯»å– Source Lot æ–‡ä»¶è·å– Part Type å’Œ quantityï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
            source_lot_df = None
            part_type_col = None
            quantity_col = None
            
            if source_lot_file_path and source_lot_file_path.exists():
                LOGGER.info(f"è¯»å– Source Lot æ–‡ä»¶: {source_lot_file_path}")
                source_lot_df = read_excel_file(source_lot_file_path)
                LOGGER.info(f"  - åŒ…å« {len(source_lot_df)} è¡Œæ•°æ®")
                
                # æŸ¥æ‰¾åˆ—å
                # Part Type åˆ—
                for col in source_lot_df.columns:
                    col_upper = str(col).strip().upper()
                    if col_upper in ['PART TYPE', 'PARTTYPE', 'PART_TYPE']:
                        part_type_col = col
                        break
                
                # Quantity åˆ—
                for col in source_lot_df.columns:
                    col_upper = str(col).strip().upper()
                    if col_upper in ['QUANTITY', 'QTY', 'QTY.', 'COUNT']:
                        quantity_col = col
                        break
            else:
                # å¦‚æœæ²¡æœ‰ source_lot_fileï¼Œå°è¯•ä» units_df ä¸­è·å– Part Type å’Œ Quantity
                LOGGER.info("æœªæä¾› Source Lot æ–‡ä»¶ï¼Œå°è¯•ä» units_df ä¸­è·å– Part Type å’Œ Quantity ä¿¡æ¯...")
                for col in units_df.columns:
                    col_upper = str(col).strip().upper()
                    if col_upper in ['PART TYPE', 'PARTTYPE', 'PART_TYPE'] and part_type_col is None:
                        part_type_col = col
                    if col_upper in ['QUANTITY', 'QTY', 'QTY.', 'COUNT'] and quantity_col is None:
                        quantity_col = col
                
                if part_type_col or quantity_col:
                    LOGGER.info(f"  ä» units_df ä¸­æ‰¾åˆ°åˆ—: Part Type={part_type_col}, Quantity={quantity_col}")
                    source_lot_df = units_df  # ä½¿ç”¨ units_df ä½œä¸ºæ•°æ®æº
            
            # Unit Name åˆ—ï¼ˆåœ¨ available_units_export å’Œ validation ä¸­ï¼‰
            unit_name_col_available = None
            for col in available_units_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['UNIT NAME', 'UNITNAME', 'UNIT_NAME', 'UNIT', 'UNITS', 'UNIT ID', 'UNITID']:
                    unit_name_col_available = col
                    break
            
            unit_name_col_validation = None
            for col in validation_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['UNIT NAME', 'UNITNAME', 'UNIT_NAME', 'UNIT', 'UNITS', 'UNIT ID', 'UNITID']:
                    unit_name_col_validation = col
                    break
            
            # åˆ›å»º Source Lot -> Part Type, Quantity æ˜ å°„
            source_lot_info_map = {}
            if source_lot_df is not None:
                # é‡æ–°æŸ¥æ‰¾ Source Lot åˆ—ï¼ˆåœ¨ source_lot_df ä¸­ï¼‰
                source_lot_col_in_file = None
                for col in source_lot_df.columns:
                    col_upper = str(col).strip().upper()
                    if col_upper in ['SOURCE', 'SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT', 'SOURCELOTS', 'SOURCE LOTS']:
                        source_lot_col_in_file = col
                        LOGGER.info(f"  åœ¨ source_lot_df ä¸­æ‰¾åˆ° Source Lot åˆ—: '{col}'")
                        break
                
                if source_lot_col_in_file:
                    for _, row in source_lot_df.iterrows():
                        sl = str(row[source_lot_col_in_file]).strip() if pd.notna(row[source_lot_col_in_file]) else ''
                        if sl:
                            info = {}
                            if part_type_col and pd.notna(row.get(part_type_col)):
                                info['Part Type'] = str(row[part_type_col]).strip()
                            if quantity_col and pd.notna(row.get(quantity_col)):
                                info['Quantity'] = str(row[quantity_col]).strip()
                            if info:
                                source_lot_info_map[sl] = info
                else:
                    LOGGER.warning(f"åœ¨ source_lot_df ä¸­æœªæ‰¾åˆ° Source Lot åˆ—ã€‚å¯ç”¨åˆ—: {source_lot_df.columns.tolist()}")
            
            # åˆå¹¶æ•°æ® - ä½¿ç”¨ Unit Name ä½œä¸ºé”®æ¥åˆå¹¶ä¸¤å¼ è¡¨çš„æ‰€æœ‰åˆ—
            # é¦–å…ˆåˆ›å»º available_units_df çš„ç´¢å¼•ï¼ˆä»¥ Unit Name ä¸ºé”®ï¼‰
            available_units_dict = {}
            if unit_name_col_available:
                for _, avail_row in available_units_df.iterrows():
                    unit_name = str(avail_row[unit_name_col_available]).strip() if pd.notna(avail_row.get(unit_name_col_available)) else ''
                    if unit_name:
                        # å­˜å‚¨è¯¥ unit çš„æ‰€æœ‰ä¿¡æ¯
                        available_units_dict[unit_name] = avail_row.to_dict()
            
            # ç¡®å®šä¸¤å¼ è¡¨çš„å…±åŒåˆ—å’Œç‹¬æœ‰åˆ—
            validation_cols = set(validation_df.columns)
            available_cols = set(available_units_df.columns)
            common_cols = validation_cols & available_cols
            validation_only_cols = validation_cols - available_cols
            available_only_cols = available_cols - validation_cols
            
            merged_list = []
            
            # ä» validation_df å¼€å§‹ï¼Œå› ä¸ºå®ƒåŒ…å«äº† source lot ä¿¡æ¯
            for _, val_row in validation_df.iterrows():
                unit_name = str(val_row[unit_name_col_validation]).strip() if unit_name_col_validation and pd.notna(val_row.get(unit_name_col_validation)) else ''
                source_lot_str = str(val_row.get('Source Lot', '')).strip() if pd.notna(val_row.get('Source Lot')) else ''
                
                if not unit_name:
                    continue
                
                # å¦‚æœ source lot åŒ…å«å¤šä¸ªï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
                if ',' in source_lot_str:
                    source_lot_str = source_lot_str.split(',')[0].strip()
                
                # åˆ›å»ºåˆå¹¶è¡Œï¼ŒåŒ…å«ä¸¤å¼ è¡¨çš„æ‰€æœ‰åˆ—
                merged_row = {}
                
                # å¤„ç†å…±åŒåˆ—ï¼šä¼˜å…ˆä½¿ç”¨ available_units_df çš„å€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ validation_df çš„å€¼
                for col in common_cols:
                    if unit_name in available_units_dict:
                        avail_val = available_units_dict[unit_name].get(col)
                        if pd.notna(avail_val) and str(avail_val).strip():
                            merged_row[col] = avail_val
                        else:
                            merged_row[col] = val_row.get(col) if pd.notna(val_row.get(col)) else ''
                    else:
                        merged_row[col] = val_row.get(col) if pd.notna(val_row.get(col)) else ''
                
                # æ·»åŠ  validation_df ç‹¬æœ‰çš„åˆ—
                for col in validation_only_cols:
                    merged_row[col] = val_row.get(col) if pd.notna(val_row.get(col)) else ''
                
                # æ·»åŠ  available_units_df ç‹¬æœ‰çš„åˆ—ï¼ˆå¦‚æœè¯¥ unit å­˜åœ¨ï¼‰
                if unit_name in available_units_dict:
                    avail_data = available_units_dict[unit_name]
                    for col in available_only_cols:
                        merged_row[col] = avail_data.get(col) if pd.notna(avail_data.get(col)) else ''
                else:
                    # å¦‚æœ available_units_df ä¸­æ²¡æœ‰è¯¥ unitï¼Œå¡«å……ç©ºå€¼
                    for col in available_only_cols:
                        merged_row[col] = ''
                
                # ç¡®ä¿ Source Lot, Part Type, Quantity, MIR åˆ—å­˜åœ¨ï¼ˆè¿™äº›æ˜¯æ±‡æ€»ä¿¡æ¯ï¼‰
                merged_row['Source Lot'] = source_lot_str if source_lot_str else 'N/A'
                merged_row['Part Type'] = source_lot_info_map.get(source_lot_str, {}).get('Part Type', '')
                merged_row['Quantity'] = source_lot_info_map.get(source_lot_str, {}).get('Quantity', '')
                merged_row['MIR'] = ''  # MIRå°†åœ¨æäº¤åæ›´æ–°
                
                merged_list.append(merged_row)
            
            # å¦‚æœ available_units_df ä¸­æœ‰é¢å¤–çš„ unitsï¼ˆä¸åœ¨ validation ä¸­ï¼‰ï¼Œä¹Ÿæ·»åŠ è¿›å»
            if unit_name_col_available:
                validation_units_set = set()
                if unit_name_col_validation:
                    for _, val_row in validation_df.iterrows():
                        unit = str(val_row[unit_name_col_validation]).strip() if pd.notna(val_row.get(unit_name_col_validation)) else ''
                        if unit:
                            validation_units_set.add(unit)
                
                for _, avail_row in available_units_df.iterrows():
                    unit_name = str(avail_row[unit_name_col_available]).strip() if pd.notna(avail_row.get(unit_name_col_available)) else ''
                    if unit_name and unit_name not in validation_units_set:
                        # è¿™ä¸ª unit åœ¨ available ä¸­ä½†ä¸åœ¨ validation ä¸­ï¼Œæ·»åŠ å®ƒ
                        merged_row = {}
                        
                        # å¤„ç†å…±åŒåˆ—ï¼šä½¿ç”¨ available_units_df çš„å€¼
                        for col in common_cols:
                            merged_row[col] = avail_row.get(col) if pd.notna(avail_row.get(col)) else ''
                        
                        # validation_df ç‹¬æœ‰çš„åˆ—å¡«å……ç©ºå€¼
                        for col in validation_only_cols:
                            merged_row[col] = ''
                        
                        # æ·»åŠ  available_units_df ç‹¬æœ‰çš„åˆ—
                        for col in available_only_cols:
                            merged_row[col] = avail_row.get(col) if pd.notna(avail_row.get(col)) else ''
                        
                        # æ·»åŠ æ±‡æ€»ä¿¡æ¯
                        merged_row['Source Lot'] = 'N/A'
                        merged_row['Part Type'] = ''
                        merged_row['Quantity'] = ''
                        merged_row['MIR'] = ''
                        
                        merged_list.append(merged_row)
            
            # åˆ›å»ºæ±‡æ€»è¡¨ï¼ˆä» available_units_export æ–‡ä»¶ä¸­è¯»å–ä¿¡æ¯ï¼‰
            summary_list = []
            
            # ä» available_units_export æ–‡ä»¶ä¸­æŸ¥æ‰¾ Source Lot, Part Type, Quantity åˆ—
            source_lot_col_available = None
            part_type_col_available = None
            quantity_col_available = None
            
            for col in available_units_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['SOURCE', 'SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT', 'SOURCELOTS', 'SOURCE LOTS'] and source_lot_col_available is None:
                    source_lot_col_available = col
                elif col_upper in ['PART TYPE', 'PARTTYPE', 'PART_TYPE'] and part_type_col_available is None:
                    part_type_col_available = col
                elif col_upper in ['QUANTITY', 'QTY', 'QTY.', 'COUNT'] and quantity_col_available is None:
                    quantity_col_available = col
            
            LOGGER.info(f"ä» available_units_export æ–‡ä»¶ä¸­æ‰¾åˆ°åˆ—: Source Lot={source_lot_col_available}, Part Type={part_type_col_available}, Quantity={quantity_col_available}")
            
            # ä» available_units_export æ–‡ä»¶ä¸­æŒ‰ Source Lot åˆ†ç»„æå–ä¿¡æ¯
            source_lot_info_from_export = {}
            if source_lot_col_available:
                grouped = available_units_df.groupby(source_lot_col_available)
                for source_lot, group_df in grouped:
                    if pd.isna(source_lot) or str(source_lot).strip() == '' or str(source_lot).strip() == 'N/A':
                        continue
                    
                    source_lot_str = str(source_lot).strip()
                    # å¦‚æœ source lot åŒ…å«å¤šä¸ªï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
                    if ',' in source_lot_str:
                        source_lot_str = source_lot_str.split(',')[0].strip()
                    
                    # ä» group_df ä¸­è·å– Part Typeï¼ˆå–ç¬¬ä¸€ä¸ªéç©ºå€¼ï¼‰
                    part_type = ''
                    if part_type_col_available:
                        part_type_values = group_df[part_type_col_available].dropna()
                        if not part_type_values.empty:
                            part_type = str(part_type_values.iloc[0]).strip()
                    
                    # Quantity æ˜¯è¯¥ source lot åœ¨ available_units_export ä¸­çš„ units æ•°é‡ï¼ˆç»Ÿè®¡æ•°é‡ï¼‰
                    quantity = len(group_df)  # ç»Ÿè®¡è¯¥ source lot æœ‰å¤šå°‘ä¸ªå¯ç”¨çš„ units
                    
                    source_lot_info_from_export[source_lot_str] = {
                        'Part Type': part_type,
                        'Quantity': quantity,
                    }
            else:
                LOGGER.warning("åœ¨ available_units_export æ–‡ä»¶ä¸­æœªæ‰¾åˆ° Source Lot åˆ—ï¼Œå°è¯•ä» validation_df ä¸­è·å–...")
                # å¦‚æœ available_units_export ä¸­æ²¡æœ‰ Source Lot åˆ—ï¼Œä» validation_df ä¸­è·å–
                if not validation_df.empty and 'Source Lot' in validation_df.columns:
                    grouped = validation_df.groupby('Source Lot')
                    for source_lot, group_df in grouped:
                        if pd.isna(source_lot) or str(source_lot).strip() == '' or str(source_lot).strip() == 'N/A':
                            continue
                        
                        source_lot_str = str(source_lot).strip()
                        if ',' in source_lot_str:
                            source_lot_str = source_lot_str.split(',')[0].strip()
                        
                        # ä» source_lot_info_map ä¸­è·å– Part Type å’Œ Quantityï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        part_type = source_lot_info_map.get(source_lot_str, {}).get('Part Type', '')
                        quantity = source_lot_info_map.get(source_lot_str, {}).get('Quantity', '')
                        total_units = len(group_df)
                        
                        source_lot_info_from_export[source_lot_str] = {
                            'Part Type': part_type,
                            'Quantity': quantity,
                            'Total Units': total_units
                        }
            
            # ä» validation_df ä¸­è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆæŒ‰ Source Lot åˆ†ç»„ï¼‰
            validation_stats = {}
            if not validation_df.empty and 'Source Lot' in validation_df.columns:
                grouped = validation_df.groupby('Source Lot')
                for source_lot, group_df in grouped:
                    if pd.isna(source_lot) or str(source_lot).strip() == '' or str(source_lot).strip() == 'N/A':
                        continue
                    
                    source_lot_str = str(source_lot).strip()
                    # å¦‚æœ source lot åŒ…å«å¤šä¸ªï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
                    if ',' in source_lot_str:
                        source_lot_str = source_lot_str.split(',')[0].strip()
                    
                    # ç»Ÿè®¡ Units æ•°é‡
                    total_units = len(group_df)
                    matched_units = len(group_df[group_df.get('Status', '') == 'Matched']) if 'Status' in group_df.columns else total_units
                    
                    validation_stats[source_lot_str] = {
                        'Total Units': total_units,
                        'Matched Units': matched_units,
                        'Missing Units': total_units - matched_units
                    }
            
            # ç”Ÿæˆæ±‡æ€»è¡¨ï¼šä» available_units_export æ–‡ä»¶ä¸­è·å–çš„ä¿¡æ¯
            for source_lot_str, info in source_lot_info_from_export.items():
                summary_list.append({
                    'Source Lot': source_lot_str,
                    'Part Type': info.get('Part Type', ''),
                    'Quantity': info.get('Quantity', ''),
                    'MIR': '',  # MIRå°†åœ¨æäº¤åæ›´æ–°
                })
            
            # å¦‚æœ validation_df ä¸­æœ‰ Source Lot ä¸åœ¨ available_units_export ä¸­ï¼Œä¹Ÿæ·»åŠ è¿›å»
            for source_lot_str in validation_stats.keys():
                if source_lot_str not in source_lot_info_from_export:
                    # å°è¯•ä» source_lot_info_map è·å–ä¿¡æ¯
                    part_type = source_lot_info_map.get(source_lot_str, {}).get('Part Type', '')
                    quantity = source_lot_info_map.get(source_lot_str, {}).get('Quantity', '')
                    
                    summary_list.append({
                        'Source Lot': source_lot_str,
                        'Part Type': part_type,
                        'Quantity': quantity,
                        'MIR': '',  # MIRå°†åœ¨æäº¤åæ›´æ–°
                    })
            
            summary_df = pd.DataFrame(summary_list)
            if not summary_df.empty:
                summary_df = summary_df.sort_values(by='Source Lot', ascending=True)
            
            # åˆ›å»ºå•ç‹¬çš„æ±‡æ€»è¡¨ï¼ˆåªåŒ…å« Source Lot, Part Type, Quantity, MIRï¼‰
            summary_simple_df = pd.DataFrame()
            if not summary_df.empty:
                # ç¡®ä¿åªåŒ…å«è¿™4åˆ—ï¼Œå¦‚æœåˆ—ä¸å­˜åœ¨åˆ™åˆ›å»ºç©ºåˆ—
                summary_simple_df = pd.DataFrame()
                summary_simple_df['Source Lot'] = summary_df['Source Lot'] if 'Source Lot' in summary_df.columns else ''
                summary_simple_df['Part Type'] = summary_df['Part Type'] if 'Part Type' in summary_df.columns else ''
                summary_simple_df['Quantity'] = summary_df['Quantity'] if 'Quantity' in summary_df.columns else ''
                summary_simple_df['MIR'] = summary_df['MIR'] if 'MIR' in summary_df.columns else ''
            
            # ä¿å­˜åˆ°åˆå¹¶Excelæ–‡ä»¶ï¼ˆå°†ä¸¤å¼ è¡¨ä½œä¸ºä¸åŒçš„å·¥ä½œè¡¨ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            merged_output_file = self.work_subdirs['mir'] / f"Merged_Validation_Table_{timestamp}.xlsx"
            
            # ä¿å­˜æ±‡æ€»è¡¨æ–‡ä»¶è·¯å¾„
            summary_output_file = self.work_subdirs['mir'] / f"Summary_Table_{timestamp}.xlsx"
            self.summary_table_file = summary_output_file
            
            # ä¿å­˜åˆå¹¶è¡¨è·¯å¾„ï¼Œä»¥ä¾¿åç»­æ›´æ–°MIR
            self.merged_validation_file = merged_output_file
            
            try:
                # ä¿å­˜åˆå¹¶è¡¨ï¼ˆåŒ…å«ä¸¤å¼ åŸå§‹è¡¨ä½œä¸ºä¸åŒçš„å·¥ä½œè¡¨ï¼‰
                with pd.ExcelWriter(merged_output_file, engine='openpyxl') as writer:
                    # å·¥ä½œè¡¨1: available_units_export è¡¨
                    available_units_df.to_excel(writer, sheet_name='Available Units Export', index=False)
                    LOGGER.info(f"  - å·¥ä½œè¡¨1 'Available Units Export': {len(available_units_df)} è¡Œ")
                    
                    # å·¥ä½œè¡¨2: Units_Validation_Comparison è¡¨ï¼ˆAll Units Comparison sheetï¼‰
                    validation_df.to_excel(writer, sheet_name='Units Validation Comparison', index=False)
                    LOGGER.info(f"  - å·¥ä½œè¡¨2 'Units Validation Comparison': {len(validation_df)} è¡Œ")
                    
                    # å¦‚æœ Units_Validation_Comparison æ–‡ä»¶æœ‰å¤šä¸ªå·¥ä½œè¡¨ï¼Œä¹Ÿæ·»åŠ è¿›å»
                    if self.units_validation_comparison_file and self.units_validation_comparison_file.suffix.lower() == '.xlsx':
                        try:
                            # è¯»å– Source Lot Summary sheet
                            source_lot_stats_df = pd.read_excel(self.units_validation_comparison_file, sheet_name='Source Lot Summary')
                            source_lot_stats_df.to_excel(writer, sheet_name='Source Lot Summary', index=False)
                            LOGGER.info(f"  - å·¥ä½œè¡¨3 'Source Lot Summary': {len(source_lot_stats_df)} è¡Œ")
                        except:
                            pass
                        
                        try:
                            # è¯»å– Missing Units sheet
                            missing_df = pd.read_excel(self.units_validation_comparison_file, sheet_name='Missing Units')
                            if not missing_df.empty:
                                missing_df.to_excel(writer, sheet_name='Missing Units', index=False)
                                LOGGER.info(f"  - å·¥ä½œè¡¨4 'Missing Units': {len(missing_df)} è¡Œ")
                        except:
                            pass
                        
                        try:
                            # è¯»å– Extra Units sheet
                            extra_df = pd.read_excel(self.units_validation_comparison_file, sheet_name='Extra Units')
                            if not extra_df.empty:
                                extra_df.to_excel(writer, sheet_name='Extra Units', index=False)
                                LOGGER.info(f"  - å·¥ä½œè¡¨5 'Extra Units': {len(extra_df)} è¡Œ")
                        except:
                            pass
                
                # è®¾ç½® Units Validation Comparison å·¥ä½œè¡¨ä¸­ Status åˆ—çš„èƒŒæ™¯é¢œè‰²
                try:
                    from openpyxl import load_workbook
                    from openpyxl.styles import PatternFill
                    
                    wb = load_workbook(merged_output_file)
                    if 'Units Validation Comparison' in wb.sheetnames:
                        ws = wb['Units Validation Comparison']
                        
                        # æŸ¥æ‰¾ Status åˆ—çš„ç´¢å¼•
                        status_col_idx = None
                        for col_idx, cell in enumerate(ws[1], 1):  # ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜è¡Œ
                            if cell.value and str(cell.value).strip().upper() == 'STATUS':
                                status_col_idx = col_idx
                                break
                        
                        if status_col_idx:
                            # å®šä¹‰é¢œè‰²å¡«å……
                            green_fill = PatternFill(start_color='90EE90', end_color='90EE90', fill_type='solid')  # æµ…ç»¿è‰²
                            red_fill = PatternFill(start_color='FFB6C1', end_color='FFB6C1', fill_type='solid')  # æµ…çº¢è‰²
                            
                            # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰è®¾ç½®èƒŒæ™¯é¢œè‰²
                            for row_idx in range(2, ws.max_row + 1):
                                cell = ws.cell(row=row_idx, column=status_col_idx)
                                status_value = str(cell.value).strip() if cell.value else ''
                                
                                if status_value.upper() == 'MATCHED':
                                    cell.fill = green_fill
                                else:
                                    cell.fill = red_fill
                            
                            wb.save(merged_output_file)
                            LOGGER.info(f"  - å·²ä¸º 'Units Validation Comparison' å·¥ä½œè¡¨çš„ Status åˆ—è®¾ç½®èƒŒæ™¯é¢œè‰²ï¼ˆMatched=ç»¿è‰²ï¼Œå…¶ä»–=çº¢è‰²ï¼‰")
                except Exception as e:
                    LOGGER.warning(f"è®¾ç½® Status åˆ—èƒŒæ™¯é¢œè‰²å¤±è´¥: {e}")
                
                LOGGER.info(f"âœ… åˆå¹¶è¡¨å·²ä¿å­˜åˆ°: {merged_output_file}")
                LOGGER.info(f"   åŒ…å«å¤šä¸ªå·¥ä½œè¡¨ï¼šAvailable Units Export, Units Validation Comparison ç­‰")
                
                # ä¿å­˜å•ç‹¬çš„æ±‡æ€»è¡¨ï¼ˆåªåŒ…å« Source Lot, Part Type, Quantity, MIRï¼‰
                if not summary_simple_df.empty:
                    summary_simple_df.to_excel(summary_output_file, index=False, engine='openpyxl')
                    LOGGER.info(f"âœ… æ±‡æ€»è¡¨å·²ä¿å­˜åˆ°: {summary_output_file}")
                    LOGGER.info(f"   åŒ…å«åˆ—: Source Lot, Part Type, Quantity, MIR ({len(summary_simple_df)} è¡Œ)")
                    LOGGER.info(f"   æ³¨æ„ï¼šMIRåˆ—å°†åœ¨æäº¤MIRåæ›´æ–°")
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼ˆå› ä¸ºæ•°æ®å·²ç»åˆå¹¶åˆ° Merged_Validation_Table ä¸­ï¼‰
                try:
                    if self.available_units_export_file and self.available_units_export_file.exists():
                        self.available_units_export_file.unlink()
                        LOGGER.debug(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {self.available_units_export_file.name}")
                    if self.units_validation_comparison_file and self.units_validation_comparison_file.exists():
                        self.units_validation_comparison_file.unlink()
                        LOGGER.debug(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {self.units_validation_comparison_file.name}")
                except Exception as e:
                    LOGGER.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
            except Exception as e:
                LOGGER.warning(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•ä¿å­˜ä¸ºCSVæ ¼å¼...")
                import traceback
                LOGGER.debug(traceback.format_exc())
                # å¦‚æœExcelä¿å­˜å¤±è´¥ï¼Œåˆ†åˆ«ä¿å­˜ä¸ºCSVæ–‡ä»¶
                merged_output_file = self.work_subdirs['mir'] / f"Merged_Validation_Table_{timestamp}.csv"
                merged_df.to_csv(merged_output_file, index=False, encoding='utf-8-sig')
                LOGGER.info(f"âœ… åˆå¹¶è¡¨å·²ä¿å­˜åˆ°: {merged_output_file} (CSVæ ¼å¼)")
                LOGGER.warning("   æ³¨æ„ï¼šCSVæ ¼å¼åªèƒ½ä¿å­˜åˆå¹¶è¡¨ï¼ŒåŸå§‹è¡¨è¯·æŸ¥çœ‹åŸå§‹æ–‡ä»¶")
            
        except Exception as e:
            LOGGER.error(f"ç«‹å³åˆå¹¶éªŒè¯è¡¨å¤±è´¥: {e}")
            import traceback
            LOGGER.error(traceback.format_exc())
    
    def _merge_validation_tables(self, source_lot_file_path: Path, mir_results: list) -> None:
        """
        åˆå¹¶ available_units_export å’Œ Units_Validation_Comparison è¡¨
        ç”ŸæˆåŒ…å« source lot, Part Type, quantity, MIR çš„åˆå¹¶è¡¨
        
        Args:
            source_lot_file_path: Source Lotæ–‡ä»¶è·¯å¾„
            mir_results: MIRç»“æœåˆ—è¡¨
        """
        try:
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not self.available_units_export_file or not self.available_units_export_file.exists():
                LOGGER.warning(f"available_units_export æ–‡ä»¶ä¸å­˜åœ¨: {self.available_units_export_file}")
                return
            
            if not self.units_validation_comparison_file or not self.units_validation_comparison_file.exists():
                LOGGER.warning(f"Units_Validation_Comparison æ–‡ä»¶ä¸å­˜åœ¨: {self.units_validation_comparison_file}")
                return
            
            LOGGER.info(f"è¯»å– available_units_export æ–‡ä»¶: {self.available_units_export_file}")
            available_units_df = read_excel_file(self.available_units_export_file)
            LOGGER.info(f"  - åŒ…å« {len(available_units_df)} è¡Œæ•°æ®")
            
            LOGGER.info(f"è¯»å– Units_Validation_Comparison æ–‡ä»¶: {self.units_validation_comparison_file}")
            # å¦‚æœæ˜¯Excelæ–‡ä»¶ï¼Œè¯»å– 'All Units Comparison' sheet
            if self.units_validation_comparison_file.suffix.lower() == '.xlsx':
                try:
                    validation_df = pd.read_excel(self.units_validation_comparison_file, sheet_name='All Units Comparison')
                except:
                    # å¦‚æœæ²¡æœ‰è¯¥sheetï¼Œè¯»å–ç¬¬ä¸€ä¸ªsheet
                    validation_df = read_excel_file(self.units_validation_comparison_file)
            else:
                validation_df = read_excel_file(self.units_validation_comparison_file)
            LOGGER.info(f"  - åŒ…å« {len(validation_df)} è¡Œæ•°æ®")
            
            # è¯»å– Source Lot æ–‡ä»¶è·å– Part Type å’Œ quantity
            LOGGER.info(f"è¯»å– Source Lot æ–‡ä»¶: {source_lot_file_path}")
            source_lot_df = read_excel_file(source_lot_file_path)
            LOGGER.info(f"  - åŒ…å« {len(source_lot_df)} è¡Œæ•°æ®")
            
            # æŸ¥æ‰¾åˆ—å
            # Source Lot åˆ—
            source_lot_col = None
            for col in source_lot_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT']:
                    source_lot_col = col
                    break
            
            # Part Type åˆ—
            part_type_col = None
            for col in source_lot_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['PART TYPE', 'PARTTYPE', 'PART_TYPE']:
                    part_type_col = col
                    break
            
            # Quantity åˆ—
            quantity_col = None
            for col in source_lot_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['QUANTITY', 'QTY', 'QTY.', 'COUNT']:
                    quantity_col = col
                    break
            
            # Unit Name åˆ—ï¼ˆåœ¨ available_units_export å’Œ validation ä¸­ï¼‰
            unit_name_col_available = None
            for col in available_units_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['UNIT NAME', 'UNITNAME', 'UNIT_NAME', 'UNIT', 'UNITS', 'UNIT ID', 'UNITID']:
                    unit_name_col_available = col
                    break
            
            unit_name_col_validation = None
            for col in validation_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['UNIT NAME', 'UNITNAME', 'UNIT_NAME', 'UNIT', 'UNITS', 'UNIT ID', 'UNITID']:
                    unit_name_col_validation = col
                    break
            
            # åˆ›å»º MIR æ˜ å°„ï¼ˆsource lot -> MIRï¼‰
            mir_map = {}
            for result in mir_results:
                source_lot = result.get('Source Lot', '')
                mir = result.get('MIR', '')
                if source_lot and mir:
                    mir_map[str(source_lot).strip()] = str(mir).strip()
            
            # åˆ›å»º Source Lot -> Part Type, Quantity æ˜ å°„
            source_lot_info_map = {}
            if source_lot_col:
                for _, row in source_lot_df.iterrows():
                    sl = str(row[source_lot_col]).strip() if pd.notna(row[source_lot_col]) else ''
                    if sl:
                        info = {}
                        if part_type_col and pd.notna(row.get(part_type_col)):
                            info['Part Type'] = str(row[part_type_col]).strip()
                        if quantity_col and pd.notna(row.get(quantity_col)):
                            info['Quantity'] = str(row[quantity_col]).strip()
                        if info:
                            source_lot_info_map[sl] = info
            
            # åˆ›å»ºæ±‡æ€»è¡¨ï¼ˆä» available_units_export æ–‡ä»¶ä¸­è¯»å–ä¿¡æ¯ï¼‰
            summary_list = []
            
            # ä» available_units_export æ–‡ä»¶ä¸­æŸ¥æ‰¾ Source Lot, Part Type, Quantity åˆ—
            source_lot_col_available = None
            part_type_col_available = None
            quantity_col_available = None
            
            for col in available_units_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['SOURCE', 'SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT', 'SOURCELOTS', 'SOURCE LOTS'] and source_lot_col_available is None:
                    source_lot_col_available = col
                elif col_upper in ['PART TYPE', 'PARTTYPE', 'PART_TYPE'] and part_type_col_available is None:
                    part_type_col_available = col
                elif col_upper in ['QUANTITY', 'QTY', 'QTY.', 'COUNT'] and quantity_col_available is None:
                    quantity_col_available = col
            
            LOGGER.info(f"ä» available_units_export æ–‡ä»¶ä¸­æ‰¾åˆ°åˆ—: Source Lot={source_lot_col_available}, Part Type={part_type_col_available}, Quantity={quantity_col_available}")
            
            # ä» available_units_export æ–‡ä»¶ä¸­æŒ‰ Source Lot åˆ†ç»„æå–ä¿¡æ¯
            source_lot_info_from_export = {}
            if source_lot_col_available:
                grouped = available_units_df.groupby(source_lot_col_available)
                for source_lot, group_df in grouped:
                    if pd.isna(source_lot) or str(source_lot).strip() == '' or str(source_lot).strip() == 'N/A':
                        continue
                    
                    source_lot_str = str(source_lot).strip()
                    # å¦‚æœ source lot åŒ…å«å¤šä¸ªï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
                    if ',' in source_lot_str:
                        source_lot_str = source_lot_str.split(',')[0].strip()
                    
                    # ä» group_df ä¸­è·å– Part Typeï¼ˆå–ç¬¬ä¸€ä¸ªéç©ºå€¼ï¼‰
                    part_type = ''
                    if part_type_col_available:
                        part_type_values = group_df[part_type_col_available].dropna()
                        if not part_type_values.empty:
                            part_type = str(part_type_values.iloc[0]).strip()
                    
                    # Quantity æ˜¯è¯¥ source lot åœ¨ available_units_export ä¸­çš„ units æ•°é‡ï¼ˆç»Ÿè®¡æ•°é‡ï¼‰
                    quantity = len(group_df)  # ç»Ÿè®¡è¯¥ source lot æœ‰å¤šå°‘ä¸ªå¯ç”¨çš„ units
                    
                    source_lot_info_from_export[source_lot_str] = {
                        'Part Type': part_type,
                        'Quantity': quantity
                    }
            else:
                LOGGER.warning("åœ¨ available_units_export æ–‡ä»¶ä¸­æœªæ‰¾åˆ° Source Lot åˆ—ï¼Œå°è¯•ä» source_lot_info_map ä¸­è·å–...")
            
            # ä» validation_df ä¸­è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆæŒ‰ Source Lot åˆ†ç»„ï¼‰
            validation_stats = {}
            if not validation_df.empty and 'Source Lot' in validation_df.columns:
                grouped = validation_df.groupby('Source Lot')
                for source_lot, group_df in grouped:
                    if pd.isna(source_lot) or str(source_lot).strip() == '' or str(source_lot).strip() == 'N/A':
                        continue
                    
                    source_lot_str = str(source_lot).strip()
                    # å¦‚æœ source lot åŒ…å«å¤šä¸ªï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
                    if ',' in source_lot_str:
                        source_lot_str = source_lot_str.split(',')[0].strip()
                    
                    # ç»Ÿè®¡ Units æ•°é‡
                    total_units = len(group_df)
                    matched_units = len(group_df[group_df.get('Status', '') == 'Matched']) if 'Status' in group_df.columns else total_units
                    
                    # è·å– MIR
                    mir = mir_map.get(source_lot_str, '')
                    
                    validation_stats[source_lot_str] = {
                        'Total Units': total_units,
                        'Matched Units': matched_units,
                        'Missing Units': total_units - matched_units,
                        'MIR': mir
                    }
            
            # ç”Ÿæˆæ±‡æ€»è¡¨ï¼šä» available_units_export æ–‡ä»¶ä¸­è·å–çš„ä¿¡æ¯
            for source_lot_str, info in source_lot_info_from_export.items():
                stats = validation_stats.get(source_lot_str, {})
                summary_list.append({
                    'Source Lot': source_lot_str,
                    'Part Type': info.get('Part Type', ''),
                    'Quantity': info.get('Quantity', ''),
                    'MIR': stats.get('MIR', ''),
                })
            
            # å¦‚æœ validation_df ä¸­æœ‰ Source Lot ä¸åœ¨ available_units_export ä¸­ï¼Œä¹Ÿæ·»åŠ è¿›å»
            for source_lot_str in validation_stats.keys():
                if source_lot_str not in source_lot_info_from_export:
                    # å°è¯•ä» source_lot_info_map è·å–ä¿¡æ¯
                    part_type = source_lot_info_map.get(source_lot_str, {}).get('Part Type', '')
                    quantity = source_lot_info_map.get(source_lot_str, {}).get('Quantity', '')
                    stats = validation_stats[source_lot_str]
                    
                    summary_list.append({
                        'Source Lot': source_lot_str,
                        'Part Type': part_type,
                        'Quantity': quantity,
                        'MIR': stats.get('MIR', ''),
                    })
            
            summary_df = pd.DataFrame(summary_list)
            if not summary_df.empty:
                summary_df = summary_df.sort_values(by='Source Lot', ascending=True)
            
            # åˆ›å»ºå•ç‹¬çš„æ±‡æ€»è¡¨ï¼ˆåªåŒ…å« Source Lot, Part Type, Quantity, MIRï¼‰
            summary_simple_df = pd.DataFrame()
            if not summary_df.empty:
                summary_simple_df = summary_df[['Source Lot', 'Part Type', 'Quantity', 'MIR']].copy()
            
            # æ›´æ–°åˆå¹¶Excelæ–‡ä»¶ï¼ˆå¦‚æœå·²å­˜åœ¨ï¼‰æˆ–åˆ›å»ºæ–°æ–‡ä»¶
            if hasattr(self, 'merged_validation_file') and self.merged_validation_file and self.merged_validation_file.exists():
                # æ›´æ–°ç°æœ‰æ–‡ä»¶
                merged_output_file = self.merged_validation_file
                LOGGER.info(f"æ›´æ–°ç°æœ‰åˆå¹¶è¡¨æ–‡ä»¶: {merged_output_file}")
            else:
                # åˆ›å»ºæ–°æ–‡ä»¶
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                merged_output_file = self.work_subdirs['mir'] / f"Merged_Validation_Table_{timestamp}.xlsx"
                self.merged_validation_file = merged_output_file
            
            # æ›´æ–°æ±‡æ€»è¡¨æ–‡ä»¶ï¼ˆå¦‚æœå·²å­˜åœ¨ï¼‰æˆ–åˆ›å»ºæ–°æ–‡ä»¶
            if hasattr(self, 'summary_table_file') and self.summary_table_file and self.summary_table_file.exists():
                summary_output_file = self.summary_table_file
                LOGGER.info(f"æ›´æ–°ç°æœ‰æ±‡æ€»è¡¨æ–‡ä»¶: {summary_output_file}")
            else:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                summary_output_file = self.work_subdirs['mir'] / f"Summary_Table_{timestamp}.xlsx"
                self.summary_table_file = summary_output_file
            
            try:
                # æ›´æ–°åˆå¹¶è¡¨ï¼ˆå°†ä¸¤å¼ è¡¨ä½œä¸ºä¸åŒçš„å·¥ä½œè¡¨ï¼‰
                with pd.ExcelWriter(merged_output_file, engine='openpyxl') as writer:
                    # å·¥ä½œè¡¨1: available_units_export è¡¨
                    available_units_df.to_excel(writer, sheet_name='Available Units Export', index=False)
                    LOGGER.info(f"  - å·¥ä½œè¡¨1 'Available Units Export': {len(available_units_df)} è¡Œ")
                    
                    # å·¥ä½œè¡¨2: Units_Validation_Comparison è¡¨ï¼ˆAll Units Comparison sheetï¼‰
                    validation_df.to_excel(writer, sheet_name='Units Validation Comparison', index=False)
                    LOGGER.info(f"  - å·¥ä½œè¡¨2 'Units Validation Comparison': {len(validation_df)} è¡Œ")
                    
                    # å¦‚æœ Units_Validation_Comparison æ–‡ä»¶æœ‰å¤šä¸ªå·¥ä½œè¡¨ï¼Œä¹Ÿæ·»åŠ è¿›å»
                    if self.units_validation_comparison_file and self.units_validation_comparison_file.suffix.lower() == '.xlsx':
                        try:
                            # è¯»å– Source Lot Summary sheet
                            source_lot_stats_df = pd.read_excel(self.units_validation_comparison_file, sheet_name='Source Lot Summary')
                            source_lot_stats_df.to_excel(writer, sheet_name='Source Lot Summary', index=False)
                            LOGGER.info(f"  - å·¥ä½œè¡¨3 'Source Lot Summary': {len(source_lot_stats_df)} è¡Œ")
                        except:
                            pass
                        
                        try:
                            # è¯»å– Missing Units sheet
                            missing_df = pd.read_excel(self.units_validation_comparison_file, sheet_name='Missing Units')
                            if not missing_df.empty:
                                missing_df.to_excel(writer, sheet_name='Missing Units', index=False)
                                LOGGER.info(f"  - å·¥ä½œè¡¨4 'Missing Units': {len(missing_df)} è¡Œ")
                        except:
                            pass
                        
                        try:
                            # è¯»å– Extra Units sheet
                            extra_df = pd.read_excel(self.units_validation_comparison_file, sheet_name='Extra Units')
                            if not extra_df.empty:
                                extra_df.to_excel(writer, sheet_name='Extra Units', index=False)
                                LOGGER.info(f"  - å·¥ä½œè¡¨5 'Extra Units': {len(extra_df)} è¡Œ")
                        except:
                            pass
                
                # è®¾ç½® Units Validation Comparison å·¥ä½œè¡¨ä¸­ Status åˆ—çš„èƒŒæ™¯é¢œè‰²
                try:
                    from openpyxl import load_workbook
                    from openpyxl.styles import PatternFill
                    
                    wb = load_workbook(merged_output_file)
                    if 'Units Validation Comparison' in wb.sheetnames:
                        ws = wb['Units Validation Comparison']
                        
                        # æŸ¥æ‰¾ Status åˆ—çš„ç´¢å¼•
                        status_col_idx = None
                        for col_idx, cell in enumerate(ws[1], 1):  # ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜è¡Œ
                            if cell.value and str(cell.value).strip().upper() == 'STATUS':
                                status_col_idx = col_idx
                                break
                        
                        if status_col_idx:
                            # å®šä¹‰é¢œè‰²å¡«å……
                            green_fill = PatternFill(start_color='90EE90', end_color='90EE90', fill_type='solid')  # æµ…ç»¿è‰²
                            red_fill = PatternFill(start_color='FFB6C1', end_color='FFB6C1', fill_type='solid')  # æµ…çº¢è‰²
                            
                            # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰è®¾ç½®èƒŒæ™¯é¢œè‰²
                            for row_idx in range(2, ws.max_row + 1):
                                cell = ws.cell(row=row_idx, column=status_col_idx)
                                status_value = str(cell.value).strip() if cell.value else ''
                                
                                if status_value.upper() == 'MATCHED':
                                    cell.fill = green_fill
                                else:
                                    cell.fill = red_fill
                            
                            wb.save(merged_output_file)
                            LOGGER.info(f"  - å·²ä¸º 'Units Validation Comparison' å·¥ä½œè¡¨çš„ Status åˆ—è®¾ç½®èƒŒæ™¯é¢œè‰²ï¼ˆMatched=ç»¿è‰²ï¼Œå…¶ä»–=çº¢è‰²ï¼‰")
                except Exception as e:
                    LOGGER.warning(f"è®¾ç½® Status åˆ—èƒŒæ™¯é¢œè‰²å¤±è´¥: {e}")
                
                LOGGER.info(f"âœ… åˆå¹¶è¡¨å·²æ›´æ–°: {merged_output_file}")
                LOGGER.info(f"   åŒ…å«å¤šä¸ªå·¥ä½œè¡¨ï¼šAvailable Units Export, Units Validation Comparison ç­‰")
                
                # æ›´æ–°æ±‡æ€»è¡¨ï¼ˆåªåŒ…å« Source Lot, Part Type, Quantity, MIRï¼‰
                if not summary_simple_df.empty:
                    summary_simple_df.to_excel(summary_output_file, index=False, engine='openpyxl')
                    LOGGER.info(f"âœ… æ±‡æ€»è¡¨å·²æ›´æ–°: {summary_output_file}")
                    LOGGER.info(f"   åŒ…å«åˆ—: Source Lot, Part Type, Quantity, MIR ({len(summary_simple_df)} è¡Œ)")
                    
                    # åœ¨å†™å…¥ MIR åï¼Œå°† Summary_Table é‡å‘½åä¸º Spark ä¼šè°ƒç”¨çš„è¡¨çš„åå­—
                    # é‡å‘½åä¸º MIR_Results_*.xlsxï¼ˆä½¿ç”¨å½“å‰æ—¶é—´æˆ³ï¼‰ï¼Œè¿™æ · Spark æ­¥éª¤å¯ä»¥è¯»å–å®ƒ
                    date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
                    spark_summary_file = self.work_subdirs['mir'] / f"MIR_Results_{date_str}.xlsx"
                    
                    try:
                        # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                        if spark_summary_file.exists():
                            spark_summary_file.unlink()
                        
                        # é‡å‘½åæ–‡ä»¶
                        summary_output_file.rename(spark_summary_file)
                        self.summary_table_file = spark_summary_file
                        LOGGER.info(f"âœ… æ±‡æ€»è¡¨å·²é‡å‘½åä¸º: {spark_summary_file.name}ï¼ˆä¾› Spark ä½¿ç”¨ï¼‰")
                    except Exception as e:
                        LOGGER.warning(f"é‡å‘½åæ±‡æ€»è¡¨å¤±è´¥: {e}ï¼Œä¿æŒåŸæ–‡ä»¶å: {summary_output_file.name}")
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼ˆå› ä¸ºæ•°æ®å·²ç»åˆå¹¶åˆ° Merged_Validation_Table ä¸­ï¼‰
                try:
                    if self.available_units_export_file and self.available_units_export_file.exists():
                        self.available_units_export_file.unlink()
                        LOGGER.debug(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {self.available_units_export_file.name}")
                    if self.units_validation_comparison_file and self.units_validation_comparison_file.exists():
                        self.units_validation_comparison_file.unlink()
                        LOGGER.debug(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {self.units_validation_comparison_file.name}")
                except Exception as e:
                    LOGGER.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
            except Exception as e:
                LOGGER.warning(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•ä¿å­˜ä¸ºCSVæ ¼å¼...")
                import traceback
                LOGGER.debug(traceback.format_exc())
                # å¦‚æœExcelä¿å­˜å¤±è´¥ï¼Œåˆ†åˆ«ä¿å­˜ä¸ºCSVæ–‡ä»¶
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                merged_output_file = self.work_subdirs['mir'] / f"Merged_Validation_Table_{timestamp}.csv"
                merged_df.to_csv(merged_output_file, index=False, encoding='utf-8-sig')
                LOGGER.info(f"âœ… åˆå¹¶è¡¨å·²ä¿å­˜åˆ°: {merged_output_file} (CSVæ ¼å¼)")
                LOGGER.warning("   æ³¨æ„ï¼šCSVæ ¼å¼åªèƒ½ä¿å­˜åˆå¹¶è¡¨ï¼ŒåŸå§‹è¡¨è¯·æŸ¥çœ‹åŸå§‹æ–‡ä»¶")
            
        except Exception as e:
            LOGGER.error(f"åˆå¹¶éªŒè¯è¡¨å¤±è´¥: {e}")
            import traceback
            LOGGER.error(traceback.format_exc())
    
    def _step_start_mole(self) -> None:
        """æ­¥éª¤0: é¢„å…ˆå¯åŠ¨Moleå·¥å…·"""
        try:
            LOGGER.info("æ­£åœ¨å¯åŠ¨Moleå·¥å…·...")
            # è°ƒç”¨_ensure_applicationæ¥å¯åŠ¨Moleå·¥å…·
            self.mole_submitter._ensure_application()
            LOGGER.info("âœ… Moleå·¥å…·å·²å¯åŠ¨")
        except Exception as e:
            raise WorkflowError(f"å¯åŠ¨Moleå·¥å…·å¤±è´¥: {e}")
    
    def _close_mole(self) -> None:
        """å…³é—­Moleå·¥å…·"""
        try:
            if not self.mole_submitter._window:
                LOGGER.info("Moleçª—å£æœªè¿æ¥ï¼Œå°è¯•æŸ¥æ‰¾å¹¶å…³é—­...")
                # å°è¯•æŸ¥æ‰¾MOLEçª—å£
                if win32gui:
                    def find_mole_window(hwnd, windows):
                        try:
                            if not win32gui.IsWindowVisible(hwnd):
                                return True
                            window_text = win32gui.GetWindowText(hwnd)
                            if "MOLE" in window_text.upper() and "LOGIN" not in window_text.upper():
                                windows.append(hwnd)
                        except:
                            pass
                        return True
                    
                    mole_windows = []
                    win32gui.EnumWindows(find_mole_window, mole_windows)
                    if mole_windows:
                        LOGGER.info(f"æ‰¾åˆ° {len(mole_windows)} ä¸ªMoleçª—å£ï¼Œå°è¯•å…³é—­...")
                        for hwnd in mole_windows:
                            try:
                                win32gui.PostMessage(hwnd, 0x0010, 0, 0)  # WM_CLOSE
                                LOGGER.info(f"å·²å‘é€å…³é—­æ¶ˆæ¯åˆ°Moleçª—å£")
                            except:
                                pass
                        time.sleep(2.0)
                        return
            
            # å¦‚æœå·²è¿æ¥çª—å£ï¼Œå°è¯•å…³é—­
            if self.mole_submitter._window:
                try:
                    self.mole_submitter._window.close()
                    LOGGER.info("âœ… å·²å…³é—­Moleçª—å£")
                    time.sleep(1.0)
                except Exception as e:
                    LOGGER.warning(f"å…³é—­Moleçª—å£å¤±è´¥: {e}ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                    # å°è¯•é€šè¿‡è¿›ç¨‹å…³é—­
                    try:
                        if win32gui and self.mole_submitter._window:
                            hwnd = self.mole_submitter._window.handle
                            win32gui.PostMessage(hwnd, 0x0010, 0, 0)  # WM_CLOSE
                            time.sleep(1.0)
                    except:
                        pass
        except Exception as e:
            LOGGER.warning(f"å…³é—­Moleå·¥å…·æ—¶å‡ºé”™: {e}")
    
    def _step_read_excel(self, excel_file_path: Path) -> pd.DataFrame:
        """æ­¥éª¤1: è¯»å–æ–‡ä»¶ï¼ˆExcelæˆ–CSVï¼‰"""
        try:
            df = read_excel_file(excel_file_path)
            file_type = "CSVæ–‡ä»¶" if excel_file_path.suffix.lower() == '.csv' else "Excelæ–‡ä»¶"
            LOGGER.info(f"âœ… æˆåŠŸè¯»å–{file_type}: {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—")
            return df
        except Exception as e:
            raise WorkflowError(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    
    def _step_submit_to_mole(self, df: pd.DataFrame, source_lot_file_path: Path, ui_config: dict = None) -> None:
        """æ­¥éª¤2: æäº¤MIRæ•°æ®åˆ°Moleå·¥å…·ï¼ˆå¾ªç¯å¤„ç†æ‰€æœ‰è¡Œï¼‰
        
        Args:
            df: DataFrame
            source_lot_file_path: Source Lotæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è·¯å¾„ï¼Œå¯èƒ½è¢«UIé…ç½®è¦†ç›–ï¼‰
            ui_config: UIé…ç½®æ•°æ®ï¼ˆå¯é€‰ï¼‰ï¼ŒåŒ…å«æœç´¢æ–¹å¼å’Œå‚æ•°
        """
        try:
            # è·å–æœç´¢æ¨¡å¼
            search_mode = ui_config.get('search_mode', 'vpos') if ui_config else 'vpos'
            
            # æ ¹æ®æœç´¢æ¨¡å¼å¤„ç†ä¸åŒçš„æ•°æ®æº
            if search_mode == 'vpos':
                # VPOs æ¨¡å¼ï¼šä» Source Lot æ–‡ä»¶è¯»å–
                # ä½¿ç”¨ UI é…ç½®çš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
                actual_source_lot_file_str = ui_config.get('source_lot_file', '') if ui_config else ''
                if not actual_source_lot_file_str:
                    actual_source_lot_file = source_lot_file_path
                else:
                    # è§£æç›¸å¯¹è·¯å¾„ï¼ˆåŸºäº workflow_automation ç›®å½•çš„çˆ¶ç›®å½•ï¼‰
                    actual_source_lot_file = Path(actual_source_lot_file_str)
                    if not actual_source_lot_file.is_absolute():
                        # ç›¸å¯¹è·¯å¾„ï¼šåŸºäº auto-vpo æ ¹ç›®å½•
                        base_dir = Path(__file__).parent.parent  # workflow_automation -> auto-vpo
                        actual_source_lot_file = (base_dir / actual_source_lot_file_str).resolve()
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not actual_source_lot_file.exists():
                    # å°è¯•æŸ¥æ‰¾æ–‡ä»¶çš„å…¶ä»–å¯èƒ½ä½ç½®
                    possible_paths = [
                        actual_source_lot_file,
                        Path(__file__).parent.parent / "input" / "Source Lot.csv",
                        Path(__file__).parent.parent / "Source Lot.csv",
                        source_lot_file_path,
                    ]
                    
                    found_file = None
                    for path in possible_paths:
                        if path.exists():
                            found_file = path
                            LOGGER.warning(f"åŸå§‹è·¯å¾„ä¸å­˜åœ¨: {actual_source_lot_file}ï¼Œä½¿ç”¨æ‰¾åˆ°çš„æ–‡ä»¶: {found_file}")
                            break
                    
                    if not found_file:
                        error_msg = (
                            f"Source Lotæ–‡ä»¶ä¸å­˜åœ¨: {actual_source_lot_file}\n"
                            f"å·²å°è¯•ä»¥ä¸‹è·¯å¾„:\n"
                            + "\n".join(f"  - {p}" for p in possible_paths)
                        )
                        raise WorkflowError(error_msg)
                    
                    actual_source_lot_file = found_file
                
                LOGGER.info(f"è¯»å–Source Lotæ–‡ä»¶: {actual_source_lot_file}")
                source_lot_df = read_excel_file(actual_source_lot_file)
                
                LOGGER.info(f"Source Lotæ–‡ä»¶åˆ—å: {source_lot_df.columns.tolist()}")
                LOGGER.info(f"Source Lotæ–‡ä»¶å…±æœ‰ {len(source_lot_df)} è¡Œæ•°æ®")
                
                # æŸ¥æ‰¾SourceLotåˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                source_lot_col = None
                for col in source_lot_df.columns:
                    col_upper = str(col).strip().upper()
                    if col_upper in ['SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT', 'SOURCELOTS', 'SOURCE LOTS']:
                        source_lot_col = col
                        LOGGER.info(f"æ‰¾åˆ°SourceLotåˆ—: '{col}'")
                        break
                
                if source_lot_col is None:
                    raise WorkflowError(f"åœ¨Source Lotæ–‡ä»¶ä¸­æœªæ‰¾åˆ°SourceLotåˆ—ã€‚å¯ç”¨åˆ—: {source_lot_df.columns.tolist()}")
                
                if source_lot_df.empty:
                    raise WorkflowError("Source Lotæ–‡ä»¶ä¸ºç©º")
                
                # å­˜å‚¨æ‰€æœ‰MIRç»“æœ
                mir_results = []
                
                # å¾ªç¯å¤„ç†æ¯ä¸€è¡Œ
                for row_index, row in source_lot_df.iterrows():
                    source_lot_value = row[source_lot_col]
                    
                    if pd.isna(source_lot_value):
                        LOGGER.warning(f"ç¬¬ {row_index + 1} è¡Œçš„SourceLotå€¼ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                    
                    source_lot_value = str(source_lot_value).strip()
                    
                    LOGGER.info("=" * 80)
                    LOGGER.info(f"å¤„ç†ç¬¬ {row_index + 1}/{len(source_lot_df)} è¡Œ: SourceLot = {source_lot_value}")
                    LOGGER.info("=" * 80)
                    
                    try:
                        # æ‰“å¼€Fileèœå• -> New MIR Request
                        LOGGER.info("å¼€å§‹Moleå·¥å…·æ“ä½œæµç¨‹...")
                        success = self.mole_submitter.submit_mir_data({})
                        
                        if success:
                            # Search By VPOsæ¨¡å¼
                            LOGGER.info("ä½¿ç”¨Search By VPOsæ¨¡å¼...")
                            # ç‚¹å‡»Search By VPOsæŒ‰é’®
                            self.mole_submitter._click_search_by_vpos_button()
                            # å¡«å†™VPOæœç´¢å¯¹è¯æ¡†
                            LOGGER.info("å¡«å†™VPOæœç´¢å¯¹è¯æ¡†...")
                            self.mole_submitter._fill_vpo_search_dialog(source_lot_value)
                            
                            # æ£€æŸ¥æœç´¢ç»“æœè¡ŒçŠ¶æ€å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ
                            # æ³¨æ„ï¼šSearch By VPOsæ¨¡å¼ä½¿ç”¨Select Available Rows
                            LOGGER.info("æ£€æŸ¥æœç´¢ç»“æœè¡ŒçŠ¶æ€...")
                            self.mole_submitter._check_row_status_and_select(ui_config, use_available_rows=True)
                            
                            # ç‚¹å‡»SubmitæŒ‰é’®
                            LOGGER.info("ç‚¹å‡»SubmitæŒ‰é’®...")
                            self.mole_submitter._click_submit_button()
                            
                            # å¤„ç†æœ€ç»ˆæˆåŠŸå¯¹è¯æ¡†å¹¶è·å–MIRå·ç 
                            LOGGER.info("å¤„ç†æœ€ç»ˆæˆåŠŸå¯¹è¯æ¡†å¹¶è·å–MIRå·ç ...")
                            
                            # å¦‚æœæ˜¯æœ€åä¸€è¡Œï¼Œå¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿copy MIRå¯¹è¯æ¡†å®Œå…¨å¼¹å‡º
                            is_last_row = (row_index == len(source_lot_df) - 1)
                            if is_last_row:
                                LOGGER.info("è¿™æ˜¯æœ€åä¸€è¡Œï¼Œç­‰å¾…æ›´é•¿æ—¶é—´ç¡®ä¿copy MIRå¯¹è¯æ¡†å®Œå…¨å¼¹å‡º...")
                                time.sleep(3.0)  # é¢å¤–ç­‰å¾…3ç§’
                            
                            mir_number = self.mole_submitter._handle_final_success_dialog_and_get_mir()
                            
                            # å¦‚æœæ˜¯æœ€åä¸€è¡Œï¼Œå†æ¬¡ç­‰å¾…ç¡®ä¿å¯¹è¯æ¡†å®Œå…¨å¤„ç†å®Œæˆ
                            if is_last_row:
                                LOGGER.info("æœ€åä¸€è¡Œå¤„ç†å®Œæˆï¼Œç­‰å¾…copy MIRå¯¹è¯æ¡†å®Œå…¨å…³é—­...")
                                time.sleep(2.0)  # å†ç­‰å¾…2ç§’ç¡®ä¿å¯¹è¯æ¡†å…³é—­
                            
                            if mir_number:
                                # ä¿å­˜è¯¥è¡Œæ•°æ®å’ŒMIRå·ç 
                                result_row = row.to_dict()
                                result_row['MIR'] = mir_number
                                mir_results.append(result_row)
                                
                                LOGGER.info(f"âœ… ç¬¬ {row_index + 1} è¡Œå¤„ç†æˆåŠŸ: SourceLot={source_lot_value}, MIR={mir_number}")
                                
                                self.results.append({
                                    'row_index': row_index,
                                    'step': 'Mole',
                                    'status': 'success',
                                    'source_lot': source_lot_value,
                                    'mir': mir_number,
                                    'timestamp': datetime.now().isoformat()
                                })
                            else:
                                LOGGER.error(f"âŒ ç¬¬ {row_index + 1} è¡Œæœªèƒ½è·å–MIRå·ç ")
                                self.errors.append({
                                    'row_index': row_index,
                                    'step': 'Mole',
                                    'error': 'æœªèƒ½è·å–MIRå·ç ',
                                    'source_lot': source_lot_value,
                                    'timestamp': datetime.now().isoformat()
                                })
                        else:
                            error_msg = f"ç¬¬ {row_index + 1} è¡ŒMoleå·¥å…·æ“ä½œå¤±è´¥"
                            LOGGER.error(f"âŒ {error_msg}")
                            self.errors.append({
                                'row_index': row_index,
                                'step': 'Mole',
                                'error': error_msg,
                                'source_lot': source_lot_value,
                                'timestamp': datetime.now().isoformat()
                            })
                    
                    except Exception as e:
                        error_msg = f"ç¬¬ {row_index + 1} è¡Œå¤„ç†å¤±è´¥: {e}"
                        LOGGER.error(f"âŒ {error_msg}")
                        LOGGER.error(traceback.format_exc())
                        self.errors.append({
                            'row_index': row_index,
                            'step': 'Mole',
                            'error': str(e),
                            'source_lot': source_lot_value,
                            'timestamp': datetime.now().isoformat()
                        })
                        # ç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                    
                    # åœ¨å¤„ç†ä¸‹ä¸€è¡Œä¹‹å‰ï¼Œç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿ç•Œé¢å‡†å¤‡å¥½
                    if row_index < len(source_lot_df) - 1:
                        LOGGER.info("ç­‰å¾…2ç§’åå¤„ç†ä¸‹ä¸€è¡Œ...")
                        time.sleep(2.0)
            
            elif search_mode == 'units':
                # Units æ¨¡å¼ï¼šä½¿ç”¨ç²˜è´´çš„ units ä¿¡æ¯
                units_info = ui_config.get('units_info', '') if ui_config else ''
                if not units_info:
                    raise WorkflowError("Units æ¨¡å¼ä¸‹å¿…é¡»æä¾› units_infoï¼ˆè¯·åœ¨é…ç½®UIä¸­ç²˜è´´Unitsä¿¡æ¯ï¼‰")
                
                LOGGER.info("ä½¿ç”¨Search By Unitsæ¨¡å¼...")
                LOGGER.info(f"Unitsä¿¡æ¯: {units_info[:100]}...")
                
                # å­˜å‚¨æ‰€æœ‰MIRç»“æœ
                mir_results = []
                
                # å¯¹äº Units æ¨¡å¼ï¼Œé€šå¸¸åªå¤„ç†ä¸€æ¬¡ï¼ˆä¸æ˜¯å¾ªç¯ï¼‰
                try:
                    # æ‰“å¼€Fileèœå• -> New MIR Request
                    LOGGER.info("å¼€å§‹Moleå·¥å…·æ“ä½œæµç¨‹...")
                    success = self.mole_submitter.submit_mir_data({})
                    
                    if success:
                        # ç‚¹å‡»Search By UnitsæŒ‰é’®
                        self.mole_submitter._click_search_by_units_button()
                        # å¡«å†™Unitsæœç´¢å¯¹è¯æ¡†
                        LOGGER.info("å¡«å†™Unitsæœç´¢å¯¹è¯æ¡†...")
                        self.mole_submitter._fill_units_search_dialog(ui_config)
                        
                        # æ£€æŸ¥æœç´¢ç»“æœè¡ŒçŠ¶æ€å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ
                        # æ³¨æ„ï¼šUnits æ¨¡å¼ä½¿ç”¨ Select Visible Rowsï¼ˆä¸æ˜¯ Select Available Rowsï¼‰
                        LOGGER.info("æ£€æŸ¥æœç´¢ç»“æœè¡ŒçŠ¶æ€...")
                        self.mole_submitter._check_row_status_and_select(ui_config, use_available_rows=False)
                        
                        # ç‚¹å‡»SubmitæŒ‰é’®
                        LOGGER.info("ç‚¹å‡»SubmitæŒ‰é’®...")
                        self.mole_submitter._click_submit_button()
                        
                        # å¤„ç†æœ€ç»ˆæˆåŠŸå¯¹è¯æ¡†å¹¶è·å–MIRå·ç 
                        LOGGER.info("å¤„ç†æœ€ç»ˆæˆåŠŸå¯¹è¯æ¡†å¹¶è·å–MIRå·ç ...")
                        # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿å¯¹è¯æ¡†å®Œå…¨å¼¹å‡º
                        LOGGER.info("ç­‰å¾…copy MIRå¯¹è¯æ¡†å®Œå…¨å¼¹å‡º...")
                        time.sleep(4.0)  # å¢åŠ ç­‰å¾…æ—¶é—´åˆ°4ç§’
                        mir_number = self.mole_submitter._handle_final_success_dialog_and_get_mir()
                        # ç­‰å¾…å¯¹è¯æ¡†å…³é—­å’Œå‰ªè´´æ¿æ›´æ–°
                        time.sleep(2.0)  # ç­‰å¾…å¯¹è¯æ¡†å…³é—­
                        
                        if mir_number:
                            mir_results.append({
                                'units_info': units_info[:50] + '...' if len(units_info) > 50 else units_info,
                                'MIR': mir_number
                            })
                            
                            LOGGER.info(f"âœ… Unitså¤„ç†æˆåŠŸ: MIR={mir_number}")
                            
                            self.results.append({
                                'step': 'Mole',
                                'status': 'success',
                                'units_info': units_info[:50] + '...' if len(units_info) > 50 else units_info,
                                'mir': mir_number,
                                'timestamp': datetime.now().isoformat()
                            })
                        else:
                            LOGGER.error("âŒ æœªèƒ½è·å–MIRå·ç ")
                            self.errors.append({
                                'step': 'Mole',
                                'error': 'æœªèƒ½è·å–MIRå·ç ',
                                'timestamp': datetime.now().isoformat()
                            })
                    else:
                        error_msg = "Moleå·¥å…·æ“ä½œå¤±è´¥"
                        LOGGER.error(f"âŒ {error_msg}")
                        self.errors.append({
                            'step': 'Mole',
                            'error': error_msg,
                            'timestamp': datetime.now().isoformat()
                        })
                
                except Exception as e:
                    error_msg = f"Unitså¤„ç†å¤±è´¥: {e}"
                    LOGGER.error(f"âŒ {error_msg}")
                    LOGGER.error(traceback.format_exc())
                    self.errors.append({
                        'step': 'Mole',
                        'error': str(e),
                        'timestamp': datetime.now().isoformat()
                    })
                
                # ä¿å­˜ Units æ¨¡å¼çš„ MIR ç»“æœ
                if mir_results:
                    LOGGER.info(f"ä¿å­˜Unitsæ¨¡å¼çš„MIRç»“æœ...")
                    self._save_all_mir_results(source_lot_file_path, mir_results)
                    # æ³¨æ„ï¼š_save_all_mir_results æ–¹æ³•å·²ç»è®¾ç½®äº† self.last_mir_result_fileï¼Œä¸éœ€è¦é‡å¤è®¾ç½®
            
            elif search_mode == 'units_by_source_lot':
                # Units by Source Lot æ¨¡å¼ï¼šä»æ–‡ä»¶è¯»å– unitsï¼ŒæŒ‰ source lot åˆ†ç»„å¤„ç†
                units_file_path = ui_config.get('units_file', '') if ui_config else ''
                if not units_file_path:
                    raise WorkflowError("Units by Source Lot æ¨¡å¼ä¸‹å¿…é¡»æä¾› units_fileï¼ˆè¯·é€‰æ‹©åŒ…å« units å’Œ source lot çš„æ–‡ä»¶ï¼‰")
                
                units_file = Path(units_file_path)
                if not units_file.is_absolute():
                    base_dir = Path(__file__).parent.parent
                    units_file = (base_dir / units_file_path).resolve()
                
                if not units_file.exists():
                    raise WorkflowError(f"Units æ–‡ä»¶ä¸å­˜åœ¨: {units_file}")
                
                LOGGER.info(f"è¯»å– Units æ–‡ä»¶: {units_file}")
                units_df = read_excel_file(units_file)
                
                LOGGER.info(f"Units æ–‡ä»¶åˆ—å: {units_df.columns.tolist()}")
                LOGGER.info(f"Units æ–‡ä»¶å…±æœ‰ {len(units_df)} è¡Œæ•°æ®")
                
                # æŸ¥æ‰¾ Source Lot åˆ—ï¼ˆæ”¯æŒ Source, SourceLot ç­‰ï¼‰
                source_lot_col = None
                for col in units_df.columns:
                    col_upper = str(col).strip().upper()
                    if col_upper in ['SOURCE', 'SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT', 'SOURCELOTS', 'SOURCE LOTS']:
                        source_lot_col = col
                        LOGGER.info(f"æ‰¾åˆ° SourceLot åˆ—: '{col}'")
                        break
                
                if source_lot_col is None:
                    raise WorkflowError(f"åœ¨ Units æ–‡ä»¶ä¸­æœªæ‰¾åˆ° SourceLot åˆ—ã€‚å¯ç”¨åˆ—: {units_df.columns.tolist()}")
                
                # æŸ¥æ‰¾ Unit Name åˆ—
                unit_name_col = None
                for col in units_df.columns:
                    col_upper = str(col).strip().upper()
                    if col_upper in ['UNIT NAME', 'UNITNAME', 'UNIT_NAME', 'UNIT', 'UNITS', 'UNIT ID', 'UNITID']:
                        unit_name_col = col
                        LOGGER.info(f"æ‰¾åˆ° Unit Name åˆ—: '{col}'")
                        break
                
                if unit_name_col is None:
                    raise WorkflowError(f"åœ¨ Units æ–‡ä»¶ä¸­æœªæ‰¾åˆ° Unit Name åˆ—ã€‚å¯ç”¨åˆ—: {units_df.columns.tolist()}")
                
                # ä½¿ç”¨å·¥ä½œç›®å½•çš„éªŒè¯å’Œå¯¼å‡ºå­ç›®å½•
                # ä½¿ç”¨ MIR ç›®å½•å­˜å‚¨ä¸´æ—¶å¯¼å‡ºå’ŒéªŒè¯æ–‡ä»¶
                temp_export_dir = self.work_subdirs['mir']
                validation_dir = self.work_subdirs['mir']
                temp_export_dir.mkdir(parents=True, exist_ok=True)
                validation_dir.mkdir(parents=True, exist_ok=True)
                LOGGER.info(f"å¯¼å‡ºæ–‡ä»¶å°†ä¿å­˜åˆ°: {temp_export_dir}")
                LOGGER.info(f"éªŒè¯ç»“æœå°†ä¿å­˜åˆ°: {validation_dir}")
                
                # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰unitsï¼ˆä¸æŒ‰source lotåˆ†ç»„ï¼‰ï¼ŒéªŒè¯å®ƒä»¬æ˜¯å¦éƒ½available
                LOGGER.info("=" * 80)
                LOGGER.info("ç¬¬ä¸€æ­¥ï¼šéªŒè¯æ‰€æœ‰unitsæ˜¯å¦availableï¼ˆä¸æŒ‰source lotåˆ†ç»„ï¼‰")
                LOGGER.info("=" * 80)
                
                # æ”¶é›†æ‰€æœ‰unitsï¼ˆå»é‡ï¼‰
                all_units_list = units_df[unit_name_col].dropna().astype(str).tolist()
                all_units_list = [u.strip() for u in all_units_list if u.strip()]
                all_units_list = list(set(all_units_list))  # å»é‡
                
                LOGGER.info(f"å…±æ”¶é›†åˆ° {len(all_units_list)} ä¸ªå”¯ä¸€çš„unitsï¼ˆå»é‡åï¼‰")
                
                # æ‰“å¼€Moleå·¥å…·è¿›è¡ŒéªŒè¯
                LOGGER.info("æ‰“å¼€Moleå·¥å…·è¿›è¡ŒéªŒè¯...")
                success = self.mole_submitter.submit_mir_data({})
                
                if not success:
                    raise WorkflowError("æ— æ³•æ‰“å¼€Moleå·¥å…·")
                
                # ç‚¹å‡»Search By UnitsæŒ‰é’®
                self.mole_submitter._click_search_by_units_button()
                
                # å¡«å†™æ‰€æœ‰unitsè¿›è¡ŒéªŒè¯
                temp_ui_config_validation = ui_config.copy() if ui_config else {}
                temp_ui_config_validation['units_info'] = '\n'.join(all_units_list)
                LOGGER.info(f"å¡«å†™æ‰€æœ‰unitsè¿›è¡ŒéªŒè¯ ({len(all_units_list)} ä¸ª)...")
                self.mole_submitter._fill_units_search_dialog(temp_ui_config_validation)
                
                # ç­‰å¾…æœç´¢ç»“æœ
                time.sleep(3.0)
                
                # å¤„ç†å¯èƒ½å‡ºç°çš„ "Inactive Source Lots" å¯¹è¯æ¡†
                LOGGER.info("ç­‰å¾…å¹¶æ£€æŸ¥ 'Inactive Source Lots' å¯¹è¯æ¡†...")
                time.sleep(2.0)
                dialog_handled = self.mole_submitter._handle_inactive_source_lots_dialog(max_wait_time=8)
                if dialog_handled:
                    LOGGER.info("âœ… å·²å¤„ç† 'Inactive Source Lots' å¯¹è¯æ¡†")
                else:
                    LOGGER.debug("æœªæ£€æµ‹åˆ° 'Inactive Source Lots' å¯¹è¯æ¡†ï¼Œç»§ç»­æ‰§è¡Œ")
                time.sleep(1.0)
                
                # å¯¼å‡ºå¹¶è·å–å®é™…å¯ç”¨çš„unitsï¼ˆä½¿ç”¨ Select Visible Rowsï¼‰
                LOGGER.info("å¯¼å‡ºå¹¶è·å–å®é™…å¯ç”¨çš„unitsï¼ˆä½¿ç”¨ Select Visible Rows åŒ…å«æ‰€æœ‰å¯è§ unitsï¼‰...")
                validation_info = self.mole_submitter._get_actual_units_count_from_export(
                    expected_units=all_units_list,
                    temp_export_dir=temp_export_dir,
                    source_lot="ALL_UNITS_VALIDATION",
                    use_visible_rows=True  # Unitsæ¨¡å¼åªèƒ½ä½¿ç”¨ Select Visible Rows
                )
                
                # è·å–å®é™…å¯ç”¨çš„unitsåˆ—è¡¨ï¼ˆä¿å­˜ä¾›åç»­ä½¿ç”¨ï¼Œä¸å†é‡å¤å¯¼å‡ºï¼‰
                available_units_list = validation_info.get('actual_units', [])
                available_units_set = set(str(u).strip() for u in available_units_list)
                validation_count = validation_info.get('actual_count', 0)
                missing_units_validation = validation_info.get('missing_units', [])
                
                # ä¿å­˜ç¬¬ä¸€æ­¥éªŒè¯çš„ç»“æœä¾›åç»­ä½¿ç”¨
                self.available_units_set = available_units_set
                
                # ä¿å­˜ç¬¬ä¸€æ­¥éªŒè¯çš„å¯¼å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¹¶ç«‹å³å†™å…¥åˆå¹¶Excelæ–‡ä»¶
                export_file = validation_info.get('export_file')
                if export_file and Path(export_file).exists():
                    self.available_units_export_file = Path(export_file)
                    LOGGER.debug(f"ä¸´æ—¶æ–‡ä»¶ï¼ˆå°†åˆå¹¶åˆ° Merged_Validation_Tableï¼‰: {self.available_units_export_file.name}")
                    
                    # ä¸å†ç«‹å³å†™å…¥ï¼Œç­‰æœ€ååˆå¹¶æ—¶ä¸€èµ·å†™å…¥
                
                # åˆ›å»ºç¬¬ä¸€æ­¥éªŒè¯çš„unitsæ¯”è¾ƒç»“æœï¼ˆåŒ…å«source lotä¿¡æ¯ï¼‰
                validation_comparison_list = []
                all_units_set = set(str(u).strip() for u in all_units_list)
                
                # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œè®°å½•æ¯ä¸ªunitå¯¹åº”çš„source lotsï¼ˆä¸€ä¸ªunitå¯èƒ½å±äºå¤šä¸ªsource lotï¼‰
                unit_to_source_lots = {}
                for _, row in units_df.iterrows():
                    unit_str = str(row[unit_name_col]).strip() if pd.notna(row[unit_name_col]) else ''
                    source_lot_str = str(row[source_lot_col]).strip() if pd.notna(row[source_lot_col]) else ''
                    if unit_str:
                        if unit_str not in unit_to_source_lots:
                            unit_to_source_lots[unit_str] = []
                        if source_lot_str and source_lot_str not in unit_to_source_lots[unit_str]:
                            unit_to_source_lots[unit_str].append(source_lot_str)
                
                # è¾“å…¥çš„unitsï¼ˆæœŸæœ›çš„ï¼‰
                for unit in all_units_list:
                    unit_str = str(unit).strip()
                    is_matched = unit_str in available_units_set
                    source_lots = ', '.join(unit_to_source_lots.get(unit_str, ['N/A']))
                    validation_comparison_list.append({
                        'Source Lot': source_lots,
                        'Unit Name': unit_str,
                        'Status': 'Matched' if is_matched else 'Missing',
                        'In_Input_File': 'Yes',
                        'In_Mole_Export': 'Yes' if is_matched else 'No'
                    })
                
                # Moleå¯¼å‡ºä¸­æœ‰ä½†è¾“å…¥æ–‡ä»¶ä¸­æ²¡æœ‰çš„unitsï¼ˆé¢å¤–å‘ç°çš„ï¼‰
                extra_units_validation = available_units_set - all_units_set
                for unit in extra_units_validation:
                    validation_comparison_list.append({
                        'Source Lot': 'N/A',
                        'Unit Name': str(unit).strip(),
                        'Status': 'Extra',
                        'In_Input_File': 'No',
                        'In_Mole_Export': 'Yes'
                    })
                
                # ä¿å­˜ç¬¬ä¸€æ­¥éªŒè¯çš„æ¯”è¾ƒç»“æœåˆ°æ–‡ä»¶
                if validation_comparison_list:
                    from datetime import datetime
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    validation_output_file = validation_dir / f"Units_Validation_Comparison_{timestamp}.xlsx"
                    
                    try:
                        validation_comparison_df = pd.DataFrame(validation_comparison_list)
                        # æŒ‰Source Lot, Statuså’ŒUnit Nameæ’åº
                        validation_comparison_df = validation_comparison_df.sort_values(
                            by=['Source Lot', 'Status', 'Unit Name'],
                            ascending=[True, True, True]
                        )
                        
                        # åˆ›å»ºSource Lotç»Ÿè®¡è¡¨
                        source_lot_stats_list = []
                        grouped_by_source_lot = units_df.groupby(source_lot_col)
                        for source_lot_value, group_df in grouped_by_source_lot:
                            if pd.isna(source_lot_value):
                                continue
                            source_lot_value = str(source_lot_value).strip()
                            
                            # è·å–è¯¥source lotçš„æ‰€æœ‰units
                            source_lot_units = group_df[unit_name_col].dropna().astype(str).tolist()
                            source_lot_units = [u.strip() for u in source_lot_units if u.strip()]
                            
                            # ç»Ÿè®¡åŒ¹é…å’Œç¼ºå¤±çš„units
                            matched_units = [u for u in source_lot_units if str(u).strip() in available_units_set]
                            missing_units = [u for u in source_lot_units if str(u).strip() not in available_units_set]
                            
                            source_lot_stats_list.append({
                                'Source Lot': source_lot_value,
                                'Total_Units': len(source_lot_units),
                                'Matched_Units': len(matched_units),
                                'Missing_Units': len(missing_units),
                                'Match_Rate': f"{len(matched_units)/len(source_lot_units)*100:.1f}%" if source_lot_units else "0.0%"
                            })
                        
                        source_lot_stats_df = pd.DataFrame(source_lot_stats_list)
                        # æŒ‰Source Lotæ’åº
                        source_lot_stats_df = source_lot_stats_df.sort_values(by='Source Lot', ascending=True)
                        
                        # ä¸å•ç‹¬ä¿å­˜ Units_Validation_Comparison æ–‡ä»¶ï¼Œæ•°æ®å°†ç›´æ¥ç”¨äºåˆå¹¶åˆ° Merged_Validation_Table
                        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºåç»­åˆå¹¶é€»è¾‘ï¼ˆåˆå¹¶åä¼šè¢«åˆ é™¤ï¼‰
                        validation_output_file = validation_dir / f"Units_Validation_Comparison_{timestamp}.xlsx"
                        
                        # å°†æ•°æ®ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆä»…ç”¨äºåç»­è¯»å–å’Œåˆå¹¶ï¼Œåˆå¹¶åä¼šè¢«åˆ é™¤ï¼‰
                        try:
                            with pd.ExcelWriter(validation_output_file, engine='openpyxl') as writer:
                                source_lot_stats_df.to_excel(writer, sheet_name='Source Lot Summary', index=False)
                                validation_comparison_df.to_excel(writer, sheet_name='All Units Comparison', index=False)
                                missing_df = validation_comparison_df[validation_comparison_df['Status'] == 'Missing'].copy()
                                if not missing_df.empty:
                                    missing_df.to_excel(writer, sheet_name='Missing Units', index=False)
                                extra_df = validation_comparison_df[validation_comparison_df['Status'] == 'Extra'].copy()
                                if not extra_df.empty:
                                    extra_df.to_excel(writer, sheet_name='Extra Units', index=False)
                            self.units_validation_comparison_file = validation_output_file
                            LOGGER.debug(f"ä¸´æ—¶æ–‡ä»¶ï¼ˆå°†åˆå¹¶åˆ° Merged_Validation_Tableï¼‰: {self.units_validation_comparison_file.name}")
                        except Exception as e:
                            LOGGER.warning(f"åˆ›å»ºä¸´æ—¶éªŒè¯æ¯”è¾ƒæ–‡ä»¶å¤±è´¥: {e}")
                        
                        # ç«‹å³åˆå¹¶ä¸¤å¼ è¡¨å¹¶ç”Ÿæˆæ±‡æ€»è¡¨ï¼ˆåœ¨tryå—å¤–ï¼Œç¡®ä¿å³ä½¿ä¿å­˜å¤±è´¥ä¹Ÿèƒ½æ‰§è¡Œï¼‰
                        if self.available_units_export_file and self.available_units_export_file.exists() and self.units_validation_comparison_file and self.units_validation_comparison_file.exists():
                            # è·å– source_lot_file_pathï¼ˆç”¨äºè¯»å– Part Type å’Œ Quantityï¼‰
                            # åœ¨ units_by_source_lot æ¨¡å¼ä¸‹ï¼Œéœ€è¦ä» units_file æˆ– source_lot_file_path è·å–
                            actual_source_lot_file = source_lot_file_path
                            if not actual_source_lot_file or not actual_source_lot_file.exists():
                                # å°è¯•ä»é…ç½®ä¸­è·å–
                                base_dir = Path(__file__).parent.parent
                                possible_paths = [
                                    base_dir / "input" / "Source Lot.csv",
                                    base_dir / "Source Lot.csv",
                                    self.config.paths.source_lot_file if hasattr(self.config, 'paths') else None
                                ]
                                for path in possible_paths:
                                    if path and Path(path).exists():
                                        actual_source_lot_file = Path(path)
                                        break
                            
                            if actual_source_lot_file and actual_source_lot_file.exists():
                                LOGGER.info("=" * 80)
                                LOGGER.info("ç«‹å³åˆå¹¶ available_units_export å’Œ Units_Validation_Comparison è¡¨...")
                                LOGGER.info("=" * 80)
                                self._merge_validation_tables_immediately(actual_source_lot_file, units_df, source_lot_col, unit_name_col)
                            else:
                                LOGGER.warning(f"æœªæ‰¾åˆ° Source Lot æ–‡ä»¶ï¼Œæ— æ³•åˆå¹¶è¡¨ã€‚å°è¯•ä½¿ç”¨ units_df ä¸­çš„ä¿¡æ¯...")
                                # å¦‚æœæ²¡æœ‰ source_lot_fileï¼Œå°è¯•ä½¿ç”¨ units_df ä¸­çš„ä¿¡æ¯
                                self._merge_validation_tables_immediately(None, units_df, source_lot_col, unit_name_col)
                        LOGGER.info(f"   æ€»è®¡: {len(validation_comparison_list)} ä¸ªunits")
                        matched_count = len([u for u in validation_comparison_list if u.get('Status') == 'Matched'])
                        missing_count = len([u for u in validation_comparison_list if u.get('Status') == 'Missing'])
                        extra_count = len([u for u in validation_comparison_list if u.get('Status') == 'Extra'])
                        LOGGER.info(f"     - åŒ¹é…: {matched_count} ä¸ª")
                        LOGGER.info(f"     - ç¼ºå¤±: {missing_count} ä¸ª")
                        LOGGER.info(f"     - é¢å¤–: {extra_count} ä¸ª")
                        LOGGER.info(f"   Source Lotç»Ÿè®¡: {len(source_lot_stats_df)} ä¸ªSource Lot")
                        total_units_by_source_lot = source_lot_stats_df['Total_Units'].sum()
                        total_matched_by_source_lot = source_lot_stats_df['Matched_Units'].sum()
                        LOGGER.info(f"     - æ€»è®¡Units: {total_units_by_source_lot} ä¸ª")
                        LOGGER.info(f"     - æ€»è®¡åŒ¹é…: {total_matched_by_source_lot} ä¸ª")
                        LOGGER.info(f"     - æ€»è®¡ç¼ºå¤±: {total_units_by_source_lot - total_matched_by_source_lot} ä¸ª")
                    except Exception as e:
                        LOGGER.warning(f"ä¿å­˜ç¬¬ä¸€æ­¥éªŒè¯æ¯”è¾ƒç»“æœå¤±è´¥: {e}")
                        # å¦‚æœExcelä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜ä¸ºCSV
                        try:
                            csv_file = validation_dir / f"Units_Validation_Comparison_{timestamp}.csv"
                            validation_comparison_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                            # ä¿å­˜éªŒè¯æ¯”è¾ƒæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºåç»­åˆå¹¶ï¼‰
                            self.units_validation_comparison_file = csv_file
                            LOGGER.info(f"âœ… ç¬¬ä¸€æ­¥éªŒè¯çš„Unitsæ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ°: {csv_file} (CSVæ ¼å¼)")
                        except Exception as e2:
                            LOGGER.error(f"ä¿å­˜CSVæ–‡ä»¶ä¹Ÿå¤±è´¥: {e2}")
                
                LOGGER.info("=" * 80)
                LOGGER.info("ç¬¬ä¸€æ­¥éªŒè¯ç»“æœ:")
                LOGGER.info(f"  æœŸæœ›Unitsæ•°é‡: {len(all_units_list)} ä¸ª")
                LOGGER.info(f"  å®é™…å¯ç”¨Unitsæ•°é‡: {validation_count} ä¸ª")
                if len(all_units_list) != validation_count:
                    LOGGER.warning(f"  å·®å¼‚: {len(all_units_list) - validation_count} ä¸ªunitsä¸å¯ç”¨")
                    if missing_units_validation:
                        LOGGER.warning(f"  ç¼ºå¤±çš„units ({len(missing_units_validation)} ä¸ª):")
                        for missing in missing_units_validation[:10]:
                            LOGGER.warning(f"    - {missing}")
                        if len(missing_units_validation) > 10:
                            LOGGER.warning(f"    ... è¿˜æœ‰ {len(missing_units_validation) - 10} ä¸ª")
                else:
                    LOGGER.info("  âœ… æ‰€æœ‰unitséƒ½å¯ç”¨")
                LOGGER.info("=" * 80)
                
                # æŒ‰ Source Lot åˆ†ç»„ï¼Œä½¿ç”¨åŸå§‹unitsæäº¤MIR
                grouped = units_df.groupby(source_lot_col)
                LOGGER.info(f"å…±æ‰¾åˆ° {len(grouped)} ä¸ªä¸åŒçš„ Source Lot")
                
                # å­˜å‚¨æ‰€æœ‰MIRç»“æœ
                mir_results = []
                
                # åˆå§‹åŒ–unitå¯¹æ¯”è¯¦æƒ…åˆ—è¡¨ï¼ˆç”¨äºä¿å­˜è¯¦ç»†çš„unitå¯¹æ¯”ä¿¡æ¯ï¼‰
                self.unit_comparison_details = []
                
                # å¾ªç¯å¤„ç†æ¯ä¸ª Source Lot ç»„
                for source_lot_value, group_df in grouped:
                    if pd.isna(source_lot_value):
                        LOGGER.warning(f"è·³è¿‡ SourceLot ä¸ºç©ºçš„ç»„")
                        continue
                    
                    source_lot_value = str(source_lot_value).strip()
                    
                    # æå–è¯¥ Source Lot çš„åŸå§‹units
                    original_units_list = group_df[unit_name_col].dropna().astype(str).tolist()
                    original_units_list = [u.strip() for u in original_units_list if u.strip()]
                    
                    if not original_units_list:
                        LOGGER.warning(f"SourceLot {source_lot_value} æ²¡æœ‰æœ‰æ•ˆçš„unitsï¼Œè·³è¿‡")
                        continue
                    
                    # ä»ç¬¬ä¸€æ­¥éªŒè¯çš„available unitsä¸­ç­›é€‰å‡ºè¯¥source lotä¸­å¯ç”¨çš„units
                    available_units_for_source_lot = [u for u in original_units_list if str(u).strip() in self.available_units_set]
                    
                    if not available_units_for_source_lot:
                        LOGGER.warning(f"SourceLot {source_lot_value} æ²¡æœ‰å¯ç”¨çš„unitsï¼ˆæ‰€æœ‰unitséƒ½ä¸å¯ç”¨ï¼‰ï¼Œè·³è¿‡")
                        continue
                    
                    LOGGER.info("=" * 80)
                    LOGGER.info(f"å¤„ç† SourceLot: {source_lot_value}")
                    LOGGER.info(f"  åŸå§‹Unitsæ•°é‡: {len(original_units_list)} ä¸ª")
                    LOGGER.info(f"  å¯ç”¨Unitsæ•°é‡: {len(available_units_for_source_lot)} ä¸ª")
                    if len(original_units_list) != len(available_units_for_source_lot):
                        missing_count = len(original_units_list) - len(available_units_for_source_lot)
                        LOGGER.warning(f"  è·³è¿‡ä¸å¯ç”¨çš„units: {missing_count} ä¸ª")
                    LOGGER.info("=" * 80)
                    
                    try:
                        # ä½¿ç”¨availableçš„unitsæœç´¢å¹¶æäº¤MIRï¼ˆç›´æ¥ä½¿ç”¨å·²æ‰“å¼€çš„Moleå·¥å…·ï¼Œä¸å†æ‰“å¼€æ–°çš„MIRè¯·æ±‚ï¼‰
                        LOGGER.info("ä½¿ç”¨availableçš„unitsæœç´¢å¹¶æäº¤MIR...")
                        LOGGER.info("ç›´æ¥ä½¿ç”¨å·²æ‰“å¼€çš„Moleå·¥å…·ï¼Œè·³è¿‡æ‰“å¼€æ–°MIRè¯·æ±‚çš„æ­¥éª¤")
                        
                        # ç›´æ¥ç‚¹å‡»Search By UnitsæŒ‰é’®ï¼ˆMoleå·¥å…·å·²ç»åœ¨ç¬¬ä¸€æ­¥éªŒè¯æ—¶æ‰“å¼€ï¼‰
                        self.mole_submitter._click_search_by_units_button()
                        
                        # å¡«å†™è¯¥source lotçš„available unitsï¼ˆåªä½¿ç”¨å¯ç”¨çš„unitsï¼‰
                        temp_ui_config_actual = ui_config.copy() if ui_config else {}
                        temp_ui_config_actual['units_info'] = '\n'.join(available_units_for_source_lot)
                        LOGGER.info(f"å¡«å†™SourceLot {source_lot_value} çš„available units ({len(available_units_for_source_lot)} ä¸ª)...")
                        self.mole_submitter._fill_units_search_dialog(temp_ui_config_actual)
                        
                        # ç­‰å¾…æœç´¢ç»“æœ
                        time.sleep(3.0)
                        
                        # å¤„ç†å¯èƒ½å‡ºç°çš„ "Inactive Source Lots" å¯¹è¯æ¡†
                        LOGGER.info("ç­‰å¾…å¹¶æ£€æŸ¥ 'Inactive Source Lots' å¯¹è¯æ¡†...")
                        time.sleep(2.0)
                        dialog_handled = self.mole_submitter._handle_inactive_source_lots_dialog(max_wait_time=8)
                        if dialog_handled:
                            LOGGER.info("âœ… å·²å¤„ç† 'Inactive Source Lots' å¯¹è¯æ¡†")
                        else:
                            LOGGER.debug("æœªæ£€æµ‹åˆ° 'Inactive Source Lots' å¯¹è¯æ¡†ï¼Œç»§ç»­æ‰§è¡Œ")
                        time.sleep(1.0)
                        
                        # ä½¿ç”¨ç¬¬ä¸€æ­¥éªŒè¯çš„ç»“æœï¼Œä¸å†é‡å¤å¯¼å‡º
                        # available_units_for_source_lot å·²ç»åœ¨ä¸Šé¢ç­›é€‰å‡ºæ¥äº†
                        expected_set = set(str(u).strip() for u in original_units_list)
                        actual_set = set(str(u).strip() for u in available_units_for_source_lot)
                        actual_count = len(available_units_for_source_lot)
                        missing_units = list(expected_set - actual_set)
                        
                        LOGGER.info(f"  è¯¥Source Lotä¸­å¯ç”¨unitsæ•°é‡: {actual_count} / {len(original_units_list)}")
                        if missing_units:
                            LOGGER.warning(f"  ç¼ºå¤±çš„units ({len(missing_units)} ä¸ª): {', '.join(missing_units[:5])}{'...' if len(missing_units) > 5 else ''}")
                        
                        # æŒ‰Unit Nameå¯¹æ¯”ï¼Œåˆ›å»ºè¯¦ç»†çš„unitså¯¹æ¯”ä¿¡æ¯
                        # ä¸ºæ¯ä¸ªè¾“å…¥çš„unitåˆ›å»ºå¯¹æ¯”è®°å½•
                        unit_comparison_list = []
                        
                        # è¾“å…¥çš„unitsï¼ˆæœŸæœ›çš„ï¼‰
                        for unit in original_units_list:
                            unit_str = str(unit).strip()
                            is_matched = unit_str in actual_set
                            unit_comparison_list.append({
                                'Source Lot': source_lot_value,
                                'Unit Name': unit_str,
                                'Status': 'Matched' if is_matched else 'Missing',
                                'In_Input': 'Yes',
                                'In_Export': 'Yes' if is_matched else 'No'
                            })
                        
                        # æ³¨æ„ï¼šä¸å†æ£€æŸ¥é¢å¤–unitsï¼Œå› ä¸ºç¬¬ä¸€æ­¥éªŒè¯å·²ç»åŒ…å«äº†æ‰€æœ‰units
                        
                        # ä¿å­˜è¯¦ç»†çš„unitså¯¹æ¯”ä¿¡æ¯ï¼ˆç”¨äºåç»­æ±‡æ€»ï¼‰
                        if not hasattr(self, 'unit_comparison_details'):
                            self.unit_comparison_details = []
                        self.unit_comparison_details.extend(unit_comparison_list)
                        
                        # æ£€æŸ¥æœç´¢ç»“æœè¡ŒçŠ¶æ€å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ
                        LOGGER.info("æ£€æŸ¥æœç´¢ç»“æœè¡ŒçŠ¶æ€...")
                        self.mole_submitter._check_row_status_and_select(
                            temp_ui_config_actual, 
                            use_available_rows=False  # Units æ¨¡å¼ä½¿ç”¨ Select Visible Rows
                        )
                        
                        # ç‚¹å‡»SubmitæŒ‰é’®
                        LOGGER.info("ç‚¹å‡»SubmitæŒ‰é’®...")
                        self.mole_submitter._click_submit_button()
                        
                        # è·å–MIRå·ç ï¼ˆå†…éƒ¨ä¼šè½®è¯¢ç­‰å¾…å¯¹è¯æ¡†å‡ºç°ï¼‰
                        LOGGER.info("ç­‰å¾…å¯¹è¯æ¡†å‡ºç°å¹¶è·å–MIRå·ç ...")
                        mir_number = self.mole_submitter._handle_final_success_dialog_and_get_mir()
                        
                        # ç­‰å¾…å¯¹è¯æ¡†å®Œå…¨å…³é—­å’Œå‰ªè´´æ¿æ›´æ–°
                        time.sleep(2.0)
                        
                        if mir_number:
                            # ä¿å­˜ç»“æœï¼ŒåªåŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼ˆé…ç½®å­—æ®µåœ¨Sparké˜¶æ®µä»For Spark.csvè¯»å–ï¼‰
                            result_row = {
                                'Source Lot': source_lot_value,
                                'Units_Count_Expected': len(original_units_list),
                                'Units_Count_Actual': actual_count,
                                'Units_Count_Diff': len(original_units_list) - actual_count,
                                'Count_Match': 'Yes' if len(original_units_list) == actual_count else 'No',
                                'Missing_Units_Count': len(missing_units),
                                'Missing_Units': ', '.join(missing_units[:5]) + ('...' if len(missing_units) > 5 else '') if missing_units else '',
                                'Units_Sample': ', '.join(original_units_list[:5]) + ('...' if len(original_units_list) > 5 else ''),
                                'MIR': mir_number,
                                'Export_File': ''  # ä¸å†å¯¼å‡ºï¼Œä½¿ç”¨ç¬¬ä¸€æ­¥éªŒè¯çš„ç»“æœ
                            }
                            mir_results.append(result_row)
                            
                            LOGGER.info(f"âœ… SourceLot {source_lot_value} å¤„ç†æˆåŠŸï¼ŒMIRå·ç å·²ä¿å­˜:")
                            LOGGER.info(f"   æœŸæœ›Unitsæ•°é‡: {len(original_units_list)} ä¸ª")
                            LOGGER.info(f"   å®é™…å¯ç”¨Unitsæ•°é‡: {actual_count} ä¸ª")
                            if len(original_units_list) != actual_count:
                                LOGGER.warning(f"   å·®å¼‚: {len(original_units_list) - actual_count} ä¸ªunitsä¸å¯ç”¨")
                            LOGGER.info(f"   MIR: {mir_number}")
                            LOGGER.info(f"   MIRå·ç å·²ä¿å­˜åˆ°ç»“æœåˆ—è¡¨ï¼Œå‡†å¤‡å¤„ç†ä¸‹ä¸€ä¸ªSource Lot...")
                            
                            self.results.append({
                                'step': 'Mole',
                                'status': 'success',
                                'source_lot': source_lot_value,
                                'units_count_expected': len(original_units_list),
                                'units_count_actual': actual_count,
                                'mir': mir_number,
                                'timestamp': datetime.now().isoformat()
                            })
                        else:
                            LOGGER.error(f"âŒ SourceLot {source_lot_value} æœªèƒ½è·å–MIRå·ç ")
                            self.errors.append({
                                'step': 'Mole',
                                'error': 'æœªèƒ½è·å–MIRå·ç ',
                                'source_lot': source_lot_value,
                                'timestamp': datetime.now().isoformat()
                            })
                    
                    except Exception as e:
                        error_msg = f"SourceLot {source_lot_value} å¤„ç†å¤±è´¥: {e}"
                        LOGGER.error(f"âŒ {error_msg}")
                        LOGGER.error(traceback.format_exc())
                        self.errors.append({
                            'step': 'Mole',
                            'error': str(e),
                            'source_lot': source_lot_value,
                            'timestamp': datetime.now().isoformat()
                        })
                    
                    # åœ¨å¤„ç†ä¸‹ä¸€ä¸ª Source Lot ä¹‹å‰ï¼Œç­‰å¾…ä¸€ä¸‹
                    if source_lot_value != list(grouped.groups.keys())[-1]:
                        LOGGER.info("ç­‰å¾…2ç§’åå¤„ç†ä¸‹ä¸€ä¸ª Source Lot...")
                        time.sleep(2.0)
                
                # ä¿å­˜ Units by Source Lot æ¨¡å¼çš„ MIR ç»“æœ
                if mir_results:
                    LOGGER.info(f"ä¿å­˜ Units by Source Lot æ¨¡å¼çš„MIRç»“æœ...")
                    self._save_all_mir_results(source_lot_file_path, mir_results)
                    
                    # åˆå¹¶ available_units_export å’Œ Units_Validation_Comparison è¡¨
                    if self.available_units_export_file and self.units_validation_comparison_file:
                        LOGGER.info("=" * 80)
                        LOGGER.info("åˆå¹¶ available_units_export å’Œ Units_Validation_Comparison è¡¨...")
                        LOGGER.info("=" * 80)
                        self._merge_validation_tables(source_lot_file_path, mir_results)
            
            else:
                raise WorkflowError(f"ä¸æ”¯æŒçš„æœç´¢æ¨¡å¼: {search_mode}")
            
            # ä¿å­˜ VPOs æ¨¡å¼çš„ MIR ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            if search_mode == 'vpos' and mir_results:
                LOGGER.info(f"ä¿å­˜VPOsæ¨¡å¼çš„MIRç»“æœ...")
                self._save_all_mir_results(source_lot_file_path, mir_results)
                # æ³¨æ„ï¼š_save_all_mir_results æ–¹æ³•å·²ç»è®¾ç½®äº† self.last_mir_result_fileï¼Œä¸éœ€è¦é‡å¤è®¾ç½®
                
                # VPOs æ¨¡å¼ï¼šç›´æ¥ç”Ÿæˆæ±‡æ€»è¡¨ï¼ˆsource lot, Part Type, quantity, MIRï¼‰
                LOGGER.info("=" * 80)
                LOGGER.info("ç”Ÿæˆ VPOs æ¨¡å¼çš„æ±‡æ€»è¡¨...")
                LOGGER.info("=" * 80)
                self._generate_summary_table_for_vpos(source_lot_file_path, mir_results)
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœç»Ÿè®¡
            LOGGER.info("=" * 80)
            LOGGER.info("Moleå¤„ç†å®Œæˆ")
            LOGGER.info("=" * 80)
            if search_mode == 'vpos':
                LOGGER.info(f"  æˆåŠŸå¤„ç†: {len(mir_results)} è¡Œ")
                LOGGER.info(f"  å¤±è´¥: {len(self.errors)} è¡Œ")
            elif search_mode == 'units_by_source_lot':
                LOGGER.info(f"  Units by Source Lot æ¨¡å¼å¤„ç†å®Œæˆ")
                LOGGER.info(f"  æˆåŠŸå¤„ç†: {len(mir_results)} ä¸ª Source Lot")
                LOGGER.info(f"  å¤±è´¥: {len(self.errors)} ä¸ª Source Lot")
            else:
                LOGGER.info(f"  Unitsæ¨¡å¼å¤„ç†å®Œæˆ")
                if mir_results:
                    LOGGER.info(f"  æˆåŠŸ: 1 ä¸ªMIR")
                else:
                    LOGGER.info(f"  å¤±è´¥: æœªèƒ½è·å–MIR")
            
            if mir_results:
                LOGGER.info(f"  ç»“æœæ–‡ä»¶: {self.last_mir_result_file}")
            LOGGER.info("=" * 80)
            
            # æ‰€æœ‰è¡Œå¤„ç†å®Œåï¼Œç­‰å¾…æ‰€æœ‰å¯¹è¯æ¡†å…³é—­ï¼Œç„¶åå…³é—­MOLE
            LOGGER.info("=" * 80)
            LOGGER.info("æ‰€æœ‰MIRæäº¤å®Œæˆï¼Œç­‰å¾…æ‰€æœ‰å¯¹è¯æ¡†å…³é—­...")
            LOGGER.info("=" * 80)
            
            # ç­‰å¾…æ‰€æœ‰æˆåŠŸå¯¹è¯æ¡†å…³é—­ï¼ˆæœ€å¤šç­‰å¾…10ç§’ï¼‰
            if win32gui:
                max_wait = 10
                for i in range(max_wait):
                    try:
                        def check_dialog(hwnd, dialogs):
                            try:
                                if not win32gui.IsWindowVisible(hwnd):
                                    return True
                                window_text = win32gui.GetWindowText(hwnd)
                                if window_text == "Submit MIR Request":
                                    dialogs.append(hwnd)
                            except:
                                pass
                            return True
                        
                        remaining_dialogs = []
                        win32gui.EnumWindows(check_dialog, remaining_dialogs)
                        if not remaining_dialogs:
                            LOGGER.info("âœ… æ‰€æœ‰å¯¹è¯æ¡†å·²å…³é—­")
                            break
                        else:
                            if i % 2 == 0:
                                LOGGER.info(f"ç­‰å¾…å¯¹è¯æ¡†å…³é—­... ({i+1}/{max_wait}ç§’ï¼Œè¿˜æœ‰{len(remaining_dialogs)}ä¸ªå¯¹è¯æ¡†)")
                            time.sleep(1.0)
                    except:
                        time.sleep(1.0)
            
            # å…³é—­MOLEå·¥å…·ï¼ˆå·²ç¦ç”¨ï¼Œç”¨æˆ·è¦æ±‚ä¸å…³é—­Moleï¼‰
            # LOGGER.info("=" * 80)
            # LOGGER.info("å…³é—­MOLEå·¥å…·...")
            # LOGGER.info("=" * 80)
            # try:
            #     self._close_mole()
            #     LOGGER.info("âœ… MOLEå·¥å…·å·²å…³é—­")
            # except Exception as e:
            #     LOGGER.warning(f"âš ï¸ å…³é—­MOLEå·¥å…·æ—¶å‡ºé”™: {e}ï¼Œç»§ç»­æ‰§è¡Œ...")
            LOGGER.info("âš ï¸ å·²è·³è¿‡å…³é—­MOLEå·¥å…·ï¼ˆç”¨æˆ·è¦æ±‚ä¿æŒMoleæ‰“å¼€ï¼‰")
            
            # æ³¨æ„ï¼šMIRç»“æœå·²ç»åœ¨ç¬¬686è¡Œï¼ˆUnitsæ¨¡å¼ï¼‰æˆ–ç¬¬695è¡Œï¼ˆVPOsæ¨¡å¼ï¼‰ä¿å­˜è¿‡äº†ï¼Œä¸éœ€è¦é‡å¤ä¿å­˜
            
            # è¾“å‡ºæ±‡æ€»ä¿¡æ¯
            LOGGER.info("=" * 80)
            if search_mode == 'vpos':
                LOGGER.info(f"å¤„ç†æ±‡æ€»:")
                LOGGER.info(f"  æ€»è¡Œæ•°: {len(source_lot_df) if 'source_lot_df' in locals() else 'N/A'}")
                LOGGER.info(f"  æˆåŠŸ: {len(mir_results)}")
                LOGGER.info(f"  å¤±è´¥: {len(self.errors)}")
            else:
                LOGGER.info(f"å¤„ç†æ±‡æ€»:")
                LOGGER.info(f"  æˆåŠŸ: {len(mir_results)}")
                LOGGER.info(f"  å¤±è´¥: {len(self.errors)}")
            LOGGER.info("=" * 80)
                
        except Exception as e:
            raise WorkflowError(f"æäº¤MIRæ•°æ®åˆ°Moleå·¥å…·å¤±è´¥: {e}")
    
    def _step_submit_to_spark(self, df: pd.DataFrame) -> None:
        """æ­¥éª¤3: æäº¤VPOæ•°æ®åˆ°Sparkç½‘é¡µï¼ˆä»MIRç»“æœæ–‡ä»¶è¯»å–æ•°æ®ï¼‰"""
        try:
            # ä¼˜å…ˆæŸ¥æ‰¾æœ¬æ¬¡è¿è¡Œå·¥ä½œç›®å½•ä¸­çš„MIRç»“æœæ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰åˆ™æŸ¥æ‰¾outputç›®å½•
            LOGGER.info("æŸ¥æ‰¾MIRç»“æœæ–‡ä»¶...")
            
            # é¦–å…ˆåœ¨å·¥ä½œç›®å½•çš„mir_resultså­ç›®å½•ä¸­æŸ¥æ‰¾ï¼ˆæ–°ç»“æ„ï¼š01_MIRï¼‰
            mir_results_dir = self.work_subdirs.get('mir', self.work_dir / '01_MIR')
            
            # ä¼˜å…ˆæŸ¥æ‰¾Excelæ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰åˆ™æŸ¥æ‰¾CSVæ–‡ä»¶
            mir_excel_files = sorted(mir_results_dir.glob("MIR_Results_*.xlsx"), reverse=True)
            mir_csv_files = sorted(mir_results_dir.glob("MIR_Results_*.csv"), reverse=True)
            
            # å¦‚æœå·¥ä½œç›®å½•ä¸­æ²¡æœ‰ï¼Œåˆ™åœ¨output/01_MIRç›®å½•ä¸­æŸ¥æ‰¾ï¼ˆæ–°ç»“æ„ï¼‰
            if not mir_excel_files and not mir_csv_files:
                output_dir = self.config.paths.output_dir
                mir_dir = output_dir / "01_MIR"
                if mir_dir.exists():
                    mir_excel_files = sorted(mir_dir.glob("MIR_Results_*.xlsx"), reverse=True)
                    mir_csv_files = sorted(mir_dir.glob("MIR_Results_*.csv"), reverse=True)
            
            # å‘åå…¼å®¹ï¼šåœ¨outputæ ¹ç›®å½•ä¸­æŸ¥æ‰¾ï¼ˆæ—§æ ¼å¼ï¼‰
            if not mir_excel_files and not mir_csv_files:
                output_dir = self.config.paths.output_dir
                if output_dir.exists():
                    mir_excel_files = sorted(output_dir.glob("MIR_Results_*.xlsx"), reverse=True)
                    mir_csv_files = sorted(output_dir.glob("MIR_Results_*.csv"), reverse=True)
            
            if not mir_excel_files and not mir_csv_files:
                raise WorkflowError(f"æœªæ‰¾åˆ°MIRç»“æœæ–‡ä»¶ã€‚è¯·å…ˆå®ŒæˆMoleæ­¥éª¤ã€‚\nå·²æ£€æŸ¥ç›®å½•: {mir_results_dir}\næ”¯æŒæ ¼å¼: .xlsx, .csv")
            
            if mir_excel_files:
                selected_file = mir_excel_files[0]
                LOGGER.info(f"ä½¿ç”¨MIRç»“æœæ–‡ä»¶ï¼ˆExcelæ ¼å¼ï¼‰: {selected_file.name}")
            elif mir_csv_files:
                selected_file = mir_csv_files[0]
                LOGGER.info(f"ä½¿ç”¨MIRç»“æœæ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼‰: {selected_file.name}")
            else:
                raise WorkflowError(f"æœªåœ¨outputç›®å½•æ‰¾åˆ°MIRç»“æœæ–‡ä»¶ï¼Œæ— æ³•æäº¤åˆ°Sparkã€‚è¯·å…ˆå®ŒæˆMoleæ­¥éª¤ã€‚\nå·²æ£€æŸ¥ç›®å½•: {output_dir}\næ”¯æŒæ ¼å¼: .xlsx, .csv")
            
            # è¯»å–MIRç»“æœæ–‡ä»¶
            mir_df = read_excel_file(selected_file)
            if mir_df.empty:
                raise WorkflowError("MIRç»“æœæ–‡ä»¶ä¸ºç©º")
            
            LOGGER.info(f"æˆåŠŸè¯»å–MIRæ•°æ®ï¼š{len(mir_df)} è¡Œ")
            LOGGER.info(f"MIRæ–‡ä»¶åˆ—å: {mir_df.columns.tolist()}")
            
            # è¯»å– For Spark.csv æ–‡ä»¶ï¼Œå»ºç«‹ Source Lot -> é…ç½®çš„æ˜ å°„
            spark_config_df = None
            spark_config_map = {}  # {source_lot: {operation, eng_id, ...}}
            
            # æŸ¥æ‰¾ For Spark.csv æ–‡ä»¶ï¼ˆåœ¨inputç›®å½•æˆ–çˆ¶ç›®å½•ï¼‰
            base_dir = Path(__file__).parent.parent
            possible_spark_config_paths = [
                base_dir / "input" / "For Spark.csv",
                base_dir / "For Spark.csv",
                self.config.paths.input_dir / "For Spark.csv"
            ]
            
            spark_config_file = None
            for path in possible_spark_config_paths:
                if path.exists():
                    spark_config_file = path
                    break
            
            # åˆå¹¶ MIR ç»“æœå’Œ For Spark.csvï¼Œç”Ÿæˆæ±‡æ€»æ–‡ä»¶
            LOGGER.info("=" * 80)
            LOGGER.info("åˆå¹¶ MIR ç»“æœå’Œ For Spark.csv...")
            LOGGER.info("=" * 80)
            
            merged_df = self._merge_mir_with_spark_config(mir_df, spark_config_file)
            
            # ä¿å­˜æ±‡æ€»æ–‡ä»¶åˆ° output æ ¹ç›®å½•ï¼ˆæ€»æ˜¯ç”Ÿæˆæ–°çš„ï¼Œç¡®ä¿æ˜¯æœ€æ–°çš„åˆå¹¶ç»“æœï¼‰
            date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            merged_file = self.work_subdirs['spark'] / f"MIR_Results_For_Spark_{date_str}.xlsx"
            
            # ç¡®ä¿ output ç›®å½•å­˜åœ¨
            self.config.paths.output_dir.mkdir(parents=True, exist_ok=True)
            
            try:
                merged_df.to_excel(merged_file, index=False, engine='openpyxl')
                LOGGER.info(f"âœ… å·²ç”Ÿæˆåˆå¹¶æ–‡ä»¶: {merged_file}")
                LOGGER.info(f"   å®Œæ•´è·¯å¾„: {merged_file.absolute()}")
                LOGGER.info(f"   åŒ…å« {len(merged_df)} è¡Œæ•°æ®")
                LOGGER.info(f"   åˆ—: {merged_df.columns.tolist()}")
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«åˆ›å»º
                if merged_file.exists():
                    file_size = merged_file.stat().st_size
                    LOGGER.info(f"   æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                else:
                    LOGGER.error(f"âŒ æ–‡ä»¶æœªæˆåŠŸåˆ›å»º: {merged_file}")
            except Exception as e:
                # å¦‚æœExcelä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜ä¸ºCSV
                LOGGER.warning(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•ä¿å­˜ä¸ºCSVæ ¼å¼...")
                import traceback
                LOGGER.debug(traceback.format_exc())
                
                merged_file = self.work_subdirs['spark'] / f"MIR_Results_For_Spark_{date_str}.csv"
                try:
                    merged_df.to_csv(merged_file, index=False, encoding='utf-8-sig')
                    LOGGER.info(f"âœ… å·²ç”Ÿæˆåˆå¹¶æ–‡ä»¶: {merged_file}")
                    LOGGER.info(f"   å®Œæ•´è·¯å¾„: {merged_file.absolute()}")
                    LOGGER.info(f"   åŒ…å« {len(merged_df)} è¡Œæ•°æ®")
                    
                    # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«åˆ›å»º
                    if merged_file.exists():
                        file_size = merged_file.stat().st_size
                        LOGGER.info(f"   æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                    else:
                        LOGGER.error(f"âŒ æ–‡ä»¶æœªæˆåŠŸåˆ›å»º: {merged_file}")
                except Exception as e2:
                    LOGGER.error(f"âŒ ä¿å­˜CSVæ–‡ä»¶ä¹Ÿå¤±è´¥: {e2}")
                    LOGGER.error(traceback.format_exc())
                    raise WorkflowError(f"æ— æ³•ä¿å­˜åˆå¹¶æ–‡ä»¶: {e2}")
            
            # ä½¿ç”¨æ±‡æ€»åçš„DataFrameç»§ç»­å¤„ç†
            mir_df = merged_df
            
            if spark_config_file:
                try:
                    LOGGER.info(f"è¯»å– Spark é…ç½®æ–‡ä»¶: {spark_config_file}")
                    spark_config_df = read_excel_file(spark_config_file)
                    
                    # æŸ¥æ‰¾ SourceLot åˆ—
                    spark_source_lot_col = None
                    for col in spark_config_df.columns:
                        col_upper = str(col).strip().upper()
                        if col_upper in ['SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT']:
                            spark_source_lot_col = col
                            break
                    
                    if spark_source_lot_col:
                        # å»ºç«‹æ˜ å°„
                        for _, row in spark_config_df.iterrows():
                            source_lot = str(row[spark_source_lot_col]).strip() if pd.notna(row[spark_source_lot_col]) else None
                            if source_lot:
                                config = {}
                                
                                # è¯»å–å„ä¸ªé…ç½®å­—æ®µ
                                for col in spark_config_df.columns:
                                    if col == spark_source_lot_col:
                                        continue
                                    col_upper = str(col).strip().upper()
                                    value = row[col]
                                    
                                    # å¤„ç†ç©ºå€¼
                                    if pd.isna(value) or str(value).strip() == '':
                                        continue
                                    
                                    value = str(value).strip()
                                    
                                    # æ˜ å°„åˆ°æ ‡å‡†å­—æ®µå
                                    if col_upper in ['OPERATION', 'OP', 'OPN']:
                                        config['operation'] = value
                                    elif col_upper in ['ENG ID', 'ENGID', 'ENG_ID', 'ENGINEERING ID', 'ENGINEERING_ID']:
                                        config['eng_id'] = value
                                    elif col_upper in ['UNIT TEST TIME', 'UNIT_TEST_TIME', 'TEST TIME']:
                                        config['unit_test_time'] = value
                                    elif col_upper in ['RETEST RATE', 'RETEST_RATE', 'RATE']:
                                        config['retest_rate'] = value
                                    elif col_upper in ['HRI / MRV', 'HRI_MRV', 'HRI', 'MRV']:
                                        config['hri_mrv'] = value
                                    elif col_upper in ['PART TYPE', 'PARTTYPE', 'PART_TYPE']:
                                        config['part_type'] = value
                                
                                spark_config_map[source_lot] = config
                                LOGGER.info(f"  åŠ è½½é…ç½®: SourceLot={source_lot}, Operation={config.get('operation', 'N/A')}, EngID={config.get('eng_id', 'N/A')}")
                        
                        LOGGER.info(f"âœ… æˆåŠŸåŠ è½½ {len(spark_config_map)} ä¸ª Source Lot çš„ Spark é…ç½®")
                    else:
                        LOGGER.warning(f"âš ï¸ åœ¨ For Spark.csv ä¸­æœªæ‰¾åˆ° SourceLot åˆ—ï¼Œè·³è¿‡åŠ è½½é…ç½®")
                except Exception as e:
                    LOGGER.warning(f"âš ï¸ è¯»å– For Spark.csv å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨MIRç»“æœæ–‡ä»¶æˆ–UIé…ç½®")
            else:
                LOGGER.info("æœªæ‰¾åˆ° For Spark.csv æ–‡ä»¶ï¼Œå°†ä½¿ç”¨MIRç»“æœæ–‡ä»¶æˆ–UIé…ç½®ä¸­çš„å€¼")
            
            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿WebDriveræ­£ç¡®å…³é—­
            with self.spark_submitter:
                # åˆå§‹åŒ–å¹¶å¯¼èˆªåˆ°é¡µé¢ï¼ˆåªéœ€è¦ä¸€æ¬¡ï¼‰
                LOGGER.info("åˆå§‹åŒ–Sparkç½‘é¡µ...")
                self.spark_submitter._init_driver()
                self.spark_submitter._navigate_to_page()
                
                # å¾ªç¯å¤„ç†æ¯ä¸€è¡ŒMIRç»“æœ
                # ä½¿ç”¨enumerateç¡®ä¿è¡Œå·ä»0å¼€å§‹ï¼Œé¿å…DataFrameç´¢å¼•é—®é¢˜
                for row_num, (idx, row) in enumerate(mir_df.iterrows()):
                    LOGGER.info("=" * 80)
                    LOGGER.info(f"å¤„ç†ç¬¬ {row_num + 1}/{len(mir_df)} è¡ŒMIRæ•°æ® (DataFrameç´¢å¼•: {idx})")
                    LOGGER.info("=" * 80)
                    
                    try:
                        # æå–æ•°æ®ï¼ˆæ”¯æŒå¤šç§åˆ—åæ ¼å¼ï¼‰
                        LOGGER.info(f"è¡Œæ•°æ®: {row.to_dict()}")
                        
                        # æŸ¥æ‰¾SourceLotåˆ—
                        source_lot = None
                        for col in row.index:
                            col_upper = str(col).strip().upper()
                            if col_upper in ['SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT']:
                                source_lot = str(row[col]).strip() if pd.notna(row[col]) else ''
                                LOGGER.info(f"æ‰¾åˆ°SourceLotåˆ—: '{col}' = '{source_lot}'")
                                break
                        
                        if not source_lot:
                            LOGGER.warning(f"ç¬¬ {row_num + 1} è¡ŒSourceLotå€¼ä¸ºç©ºï¼Œè·³è¿‡")
                            LOGGER.warning(f"å¯ç”¨åˆ—: {row.index.tolist()}")
                            continue
                        
                        # æŸ¥æ‰¾Part Typeåˆ—ï¼ˆä¼˜å…ˆçº§ï¼šFor Spark.csv > MIRç»“æœæ–‡ä»¶ï¼‰
                        part_type = None
                        
                        # ä¼˜å…ˆä» For Spark.csv è¯»å–
                        if source_lot in spark_config_map and 'part_type' in spark_config_map[source_lot]:
                            part_type = spark_config_map[source_lot]['part_type']
                            LOGGER.info(f"ä» For Spark.csv è¯»å– Part Type: {part_type}")
                        else:
                            # ä»MIRç»“æœæ–‡ä»¶è¯»å–
                            for col in row.index:
                                col_upper = str(col).strip().upper()
                                if col_upper in ['PART TYPE', 'PARTTYPE', 'PART_TYPE']:
                                    if pd.notna(row[col]) and str(row[col]).strip():
                                        part_type = str(row[col]).strip()
                                        LOGGER.info(f"ä»MIRç»“æœæ–‡ä»¶è¯»å– Part Type: {part_type}")
                                    break
                        
                        if not part_type:
                            LOGGER.warning(f"ç¬¬ {row_num + 1} è¡ŒPart Typeå€¼ä¸ºç©ºï¼Œè·³è¿‡")
                            continue
                        
                        # æŸ¥æ‰¾Operationåˆ—ï¼ˆä¼˜å…ˆçº§ï¼šFor Spark.csv > MIRç»“æœæ–‡ä»¶ > UIé…ç½®ï¼‰
                        operation = None
                        
                        # ä¼˜å…ˆçº§1: ä» For Spark.csv è¯»å–
                        if source_lot in spark_config_map and 'operation' in spark_config_map[source_lot]:
                            operation = spark_config_map[source_lot]['operation']
                            LOGGER.info(f"ä» For Spark.csv è¯»å– Operation: {operation}")
                        else:
                            # ä¼˜å…ˆçº§2: ä»MIRç»“æœæ–‡ä»¶è¯»å–
                            for col in row.index:
                                col_upper = str(col).strip().upper()
                                if col_upper in ['OPERATION', 'OP', 'OPN']:
                                    if pd.notna(row[col]) and str(row[col]).strip():
                                        operation = str(row[col]).strip()
                                        LOGGER.info(f"ä»MIRç»“æœæ–‡ä»¶è¯»å– Operation: {operation}")
                                    break
                            
                            # ä¼˜å…ˆçº§3: ä½¿ç”¨UIé…ç½®æˆ–é…ç½®æ–‡ä»¶çš„å€¼
                            if not operation:
                                if hasattr(self, 'ui_config') and self.ui_config:
                                    operation = self.ui_config.get('operation', '').strip() or None
                                    if operation:
                                        LOGGER.info(f"ä»UIé…ç½®è¯»å– Operation: {operation}")
                                else:
                                    # ä»é…ç½®æ–‡ä»¶è¯»å–
                                    try:
                                        config_path = Path(__file__).parent / "config.yaml"
                                        if config_path.exists():
                                            import yaml
                                            with open(config_path, 'r', encoding='utf-8') as f:
                                                config_data = yaml.safe_load(f) or {}
                                                mole_history = config_data.get('mole_history', {})
                                                operation = mole_history.get('operation', '').strip() or None
                                                if operation:
                                                    LOGGER.info(f"ä»é…ç½®æ–‡ä»¶è¯»å– Operation: {operation}")
                                    except Exception:
                                        pass
                        
                        # æŸ¥æ‰¾Eng IDåˆ—ï¼ˆä¼˜å…ˆçº§ï¼šFor Spark.csv > MIRç»“æœæ–‡ä»¶ > UIé…ç½®ï¼‰
                        eng_id = None
                        
                        # ä¼˜å…ˆçº§1: ä» For Spark.csv è¯»å–
                        if source_lot in spark_config_map and 'eng_id' in spark_config_map[source_lot]:
                            eng_id = spark_config_map[source_lot]['eng_id']
                            LOGGER.info(f"ä» For Spark.csv è¯»å– EngID: {eng_id}")
                        else:
                            # ä¼˜å…ˆçº§2: ä»MIRç»“æœæ–‡ä»¶è¯»å–
                            for col in row.index:
                                col_upper = str(col).strip().upper()
                                if col_upper in ['ENG ID', 'ENGID', 'ENG_ID', 'ENGINEERING ID', 'ENGINEERING_ID']:
                                    if pd.notna(row[col]) and str(row[col]).strip():
                                        eng_id = str(row[col]).strip()
                                        LOGGER.info(f"ä»MIRç»“æœæ–‡ä»¶è¯»å– EngID: {eng_id}")
                                    break
                            
                            # ä¼˜å…ˆçº§3: ä½¿ç”¨UIé…ç½®æˆ–é…ç½®æ–‡ä»¶çš„å€¼
                            if not eng_id:
                                if hasattr(self, 'ui_config') and self.ui_config:
                                    eng_id = self.ui_config.get('engid', '').strip() or None
                                    if eng_id:
                                        LOGGER.info(f"ä»UIé…ç½®è¯»å– EngID: {eng_id}")
                                else:
                                    # ä»é…ç½®æ–‡ä»¶è¯»å–
                                    try:
                                        config_path = Path(__file__).parent / "config.yaml"
                                        if config_path.exists():
                                            import yaml
                                            with open(config_path, 'r', encoding='utf-8') as f:
                                                config_data = yaml.safe_load(f) or {}
                                                mole_history = config_data.get('mole_history', {})
                                                eng_id = mole_history.get('engid', '').strip() or None
                                                if eng_id:
                                                    LOGGER.info(f"ä»é…ç½®æ–‡ä»¶è¯»å– EngID: {eng_id}")
                                    except Exception:
                                        pass
                        
                        # å¤„ç†More optionså­—æ®µï¼ˆä¼˜å…ˆçº§ï¼šFor Spark.csv > MIRç»“æœæ–‡ä»¶ > UIé…ç½®ï¼‰
                        unit_test_time = None
                        retest_rate = None
                        hri_mrv = None
                        
                        # ä¼˜å…ˆçº§1: ä» For Spark.csv è¯»å–
                        if source_lot in spark_config_map:
                            spark_config = spark_config_map[source_lot]
                            if 'unit_test_time' in spark_config:
                                unit_test_time = spark_config['unit_test_time']
                                LOGGER.info(f"ä» For Spark.csv è¯»å– Unit test time: {unit_test_time}")
                            if 'retest_rate' in spark_config:
                                retest_rate = spark_config['retest_rate']
                                LOGGER.info(f"ä» For Spark.csv è¯»å– Retest rate: {retest_rate}")
                            if 'hri_mrv' in spark_config:
                                hri_mrv = spark_config['hri_mrv']
                                LOGGER.info(f"ä» For Spark.csv è¯»å– HRI / MRV: {hri_mrv}")
                        
                        # ä¼˜å…ˆçº§2: ä»MIRç»“æœæ–‡ä»¶è¯»å–ï¼ˆå¦‚æœFor Spark.csvä¸­æ²¡æœ‰ï¼‰
                        if not unit_test_time:
                            unit_test_time = row.get('Unit test time', None)
                            if pd.notna(unit_test_time) and str(unit_test_time).strip():
                                unit_test_time = str(unit_test_time).strip()
                                LOGGER.info(f"ä»MIRç»“æœæ–‡ä»¶è¯»å– Unit test time: {unit_test_time}")
                        
                        if not retest_rate:
                            retest_rate = row.get('Retest rate', None)
                            if pd.notna(retest_rate) and str(retest_rate).strip():
                                retest_rate = str(retest_rate).strip()
                                LOGGER.info(f"ä»MIRç»“æœæ–‡ä»¶è¯»å– Retest rate: {retest_rate}")
                        
                        if not hri_mrv:
                            hri_mrv = row.get('HRI / MRV:', None)
                            if pd.notna(hri_mrv) and str(hri_mrv).strip():
                                hri_mrv = str(hri_mrv).strip()
                                LOGGER.info(f"ä»MIRç»“æœæ–‡ä»¶è¯»å– HRI / MRV: {hri_mrv}")
                        
                        # ä¼˜å…ˆçº§3: ä½¿ç”¨UIé…ç½®æˆ–é…ç½®æ–‡ä»¶çš„å€¼ï¼ˆå¦‚æœå‰é¢éƒ½æ²¡æœ‰ï¼‰
                        if not unit_test_time:
                            if hasattr(self, 'ui_config') and self.ui_config:
                                ui_value = self.ui_config.get('unit_test_time', '').strip()
                                if ui_value:
                                    unit_test_time = ui_value
                                    LOGGER.info(f"ä»UIé…ç½®è¯»å– Unit test time: {unit_test_time}")
                            else:
                                # ä»é…ç½®æ–‡ä»¶è¯»å–
                                try:
                                    config_path = Path(__file__).parent / "config.yaml"
                                    if config_path.exists():
                                        import yaml
                                        with open(config_path, 'r', encoding='utf-8') as f:
                                            config_data = yaml.safe_load(f) or {}
                                            mole_history = config_data.get('mole_history', {})
                                            ui_value = mole_history.get('unit_test_time', '').strip()
                                            if ui_value:
                                                unit_test_time = ui_value
                                                LOGGER.info(f"ä»é…ç½®æ–‡ä»¶è¯»å– Unit test time: {unit_test_time}")
                                except Exception:
                                    pass
                        
                        if not retest_rate:
                            if hasattr(self, 'ui_config') and self.ui_config:
                                ui_value = self.ui_config.get('retest_rate', '').strip()
                                if ui_value:
                                    retest_rate = ui_value
                                    LOGGER.info(f"ä»UIé…ç½®è¯»å– Retest rate: {retest_rate}")
                            else:
                                # ä»é…ç½®æ–‡ä»¶è¯»å–
                                try:
                                    config_path = Path(__file__).parent / "config.yaml"
                                    if config_path.exists():
                                        import yaml
                                        with open(config_path, 'r', encoding='utf-8') as f:
                                            config_data = yaml.safe_load(f) or {}
                                            mole_history = config_data.get('mole_history', {})
                                            ui_value = mole_history.get('retest_rate', '').strip()
                                            if ui_value:
                                                retest_rate = ui_value
                                                LOGGER.info(f"ä»é…ç½®æ–‡ä»¶è¯»å– Retest rate: {retest_rate}")
                                except Exception:
                                    pass
                        
                        if not hri_mrv:
                            if hasattr(self, 'ui_config') and self.ui_config:
                                ui_value = self.ui_config.get('hri_mrv', '').strip()
                                if ui_value:
                                    hri_mrv = ui_value
                                    LOGGER.info(f"ä»UIé…ç½®è¯»å– HRI / MRV: {hri_mrv}")
                            else:
                                # ä»é…ç½®æ–‡ä»¶è¯»å–
                                try:
                                    config_path = Path(__file__).parent / "config.yaml"
                                    if config_path.exists():
                                        import yaml
                                        with open(config_path, 'r', encoding='utf-8') as f:
                                            config_data = yaml.safe_load(f) or {}
                                            mole_history = config_data.get('mole_history', {})
                                            ui_value = mole_history.get('hri_mrv', '').strip()
                                            if ui_value:
                                                hri_mrv = ui_value
                                                LOGGER.info(f"ä»é…ç½®æ–‡ä»¶è¯»å– HRI / MRV: {hri_mrv}")
                                except Exception:
                                    pass
                        
                        # æ‰§è¡ŒSparkæäº¤æµç¨‹
                        # æ³¨æ„ï¼šç¬¬ä¸€è¡Œéœ€è¦ç‚¹å‡»Add Newï¼Œåç»­è¡Œåœ¨ä¸Šä¸€è¡ŒRollåå·²ç»ç‚¹å‡»äº†Add New
                        if row_num == 0:
                            LOGGER.info("æ­¥éª¤ 1/13: ç‚¹å‡»Add New...")
                            if not self.spark_submitter._click_add_new_button():
                                raise WorkflowError("ç‚¹å‡»Add NewæŒ‰é’®å¤±è´¥")
                        else:
                            LOGGER.info("æ­¥éª¤ 1/13: å·²ç‚¹å‡»Add Newï¼ˆä¸Šä¸€è¡ŒRollåå·²ç‚¹å‡»ï¼‰")
                        
                        LOGGER.info("æ­¥éª¤ 2/13: å¡«å†™TPè·¯å¾„...")
                        if not self.spark_submitter._fill_test_program_path(self.config.paths.tp_path):
                            raise WorkflowError("å¡«å†™TPè·¯å¾„å¤±è´¥")
                        
                        LOGGER.info("æ­¥éª¤ 3/13: ç‚¹å‡»Add New Experiment...")
                        if not self.spark_submitter._click_add_new_experiment():
                            raise WorkflowError("ç‚¹å‡»Add New Experimentå¤±è´¥")
                        
                        LOGGER.info("æ­¥éª¤ 4/13: é€‰æ‹©VPOç±»åˆ«...")
                        if not self.spark_submitter._select_vpo_category(self.config.spark.vpo_category):
                            raise WorkflowError("é€‰æ‹©VPOç±»åˆ«å¤±è´¥")
                        
                        LOGGER.info("æ­¥éª¤ 5/13: å¡«å†™å®éªŒä¿¡æ¯...")
                        if not self.spark_submitter._fill_experiment_info(self.config.spark.step, self.config.spark.tags):
                            raise WorkflowError("å¡«å†™å®éªŒä¿¡æ¯å¤±è´¥")
                        
                        LOGGER.info("æ­¥éª¤ 6/13: æ·»åŠ Lot name...")
                        # æŸ¥æ‰¾Quantityåˆ—ï¼ˆç”¨äºè®¾ç½®unitsæ•°é‡ï¼‰
                        quantity = None
                        for col in row.index:
                            col_upper = str(col).strip().upper()
                            if col_upper in ['QUANTITY', 'QTY', 'UNITS', 'UNIT COUNT', 'COUNT']:
                                if pd.notna(row[col]) and str(row[col]).strip():
                                    try:
                                        quantity = int(float(str(row[col]).strip()))
                                        LOGGER.info(f"ä»æ•°æ®ä¸­è¯»å–Quantity: {quantity}")
                                    except (ValueError, TypeError):
                                        LOGGER.warning(f"Quantityå€¼æ— æ•ˆ: {row[col]}ï¼Œè·³è¿‡è®¾ç½®unitsæ•°é‡")
                                break
                        
                        if not self.spark_submitter._add_lot_name(source_lot, quantity):
                            raise WorkflowError("æ·»åŠ Lot nameå¤±è´¥")
                        
                        LOGGER.info("æ­¥éª¤ 7/13: é€‰æ‹©Part Type...")
                        if not self.spark_submitter._select_parttype(part_type):
                            raise WorkflowError("é€‰æ‹©Part Typeå¤±è´¥")
                        
                        LOGGER.info("æ­¥éª¤ 8/13: ç‚¹å‡»Flowæ ‡ç­¾...")
                        if not self.spark_submitter._click_flow_tab():
                            raise WorkflowError("ç‚¹å‡»Flowæ ‡ç­¾å¤±è´¥")
                        
                        # Operationæ˜¯å¯é€‰çš„ï¼Œä½†å¦‚æœå­˜åœ¨åˆ™å¿…é¡»æˆåŠŸé€‰æ‹©
                        if operation:
                            LOGGER.info("æ­¥éª¤ 9/13: é€‰æ‹©Operation...")
                            if not self.spark_submitter._select_operation(operation):
                                raise WorkflowError("é€‰æ‹©Operationå¤±è´¥")
                        else:
                            LOGGER.info("æ­¥éª¤ 9/13: è·³è¿‡Operationï¼ˆæ–‡ä»¶ä¸­æœªæä¾›ï¼‰")
                        
                        # Eng IDæ˜¯å¯é€‰çš„ï¼Œä½†å¦‚æœå­˜åœ¨åˆ™å¿…é¡»æˆåŠŸé€‰æ‹©
                        if eng_id:
                            LOGGER.info("æ­¥éª¤ 10/13: é€‰æ‹©Eng ID...")
                            if not self.spark_submitter._select_eng_id(eng_id):
                                raise WorkflowError("é€‰æ‹©Eng IDå¤±è´¥")
                        else:
                            LOGGER.info("æ­¥éª¤ 10/13: è·³è¿‡Eng IDï¼ˆæ–‡ä»¶ä¸­æœªæä¾›ï¼‰")
                        
                        LOGGER.info("æ­¥éª¤ 11/13: ç‚¹å‡»More optionsæ ‡ç­¾...")
                        if not self.spark_submitter._click_more_options_tab():
                            raise WorkflowError("ç‚¹å‡»More optionsæ ‡ç­¾å¤±è´¥")
                        
                        LOGGER.info("æ­¥éª¤ 12/13: å¡«å†™More optionså­—æ®µ...")
                        if not self.spark_submitter._fill_more_options(unit_test_time, retest_rate, hri_mrv):
                            raise WorkflowError("å¡«å†™More optionså­—æ®µå¤±è´¥")
                        
                        LOGGER.info("æ­¥éª¤ 13/13: ç‚¹å‡»RollæŒ‰é’®...")
                        if not self.spark_submitter._click_roll_button():
                            raise WorkflowError("ç‚¹å‡»RollæŒ‰é’®å¤±è´¥")
                        
                        # ç­‰å¾…Rollæäº¤å®Œæˆ
                        LOGGER.info("ç­‰å¾…Rollæäº¤å®Œæˆ...")
                        time.sleep(3.0)  # ç­‰å¾…æäº¤å“åº”
                        
                        LOGGER.info(f"âœ… ç¬¬ {row_num + 1} è¡Œæ•°æ®æäº¤æˆåŠŸ")
                        self.results.append({
                            'row_index': idx,
                            'step': 'Spark',
                            'status': 'success',
                            'source_lot': source_lot,
                            'timestamp': datetime.now().isoformat()
                        })
                        
                        # å¦‚æœä¸æ˜¯æœ€åä¸€è¡Œï¼Œç‚¹å‡»Add NewæŒ‰é’®å¼€å§‹ä¸‹ä¸€è¡Œ
                        if row_num < len(mir_df) - 1:
                            LOGGER.info("=" * 80)
                            LOGGER.info(f"å‡†å¤‡å¤„ç†ä¸‹ä¸€è¡Œï¼ˆç¬¬ {row_num + 2}/{len(mir_df)} è¡Œï¼‰...")
                            LOGGER.info("ç‚¹å‡»Add NewæŒ‰é’®å¼€å§‹æ–°çš„æäº¤...")
                            LOGGER.info("=" * 80)
                            
                            # ç­‰å¾…é¡µé¢ç¨³å®š
                            time.sleep(2.0)
                            
                            # ç‚¹å‡»Add NewæŒ‰é’®
                            if not self.spark_submitter._click_add_new_button():
                                raise WorkflowError("ç‚¹å‡»Add NewæŒ‰é’®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ")
                            
                            # ç­‰å¾…Add Newå¯¹è¯æ¡†æˆ–é¡µé¢å“åº”
                            time.sleep(2.0)
                            
                            LOGGER.info("âœ… å·²ç‚¹å‡»Add NewæŒ‰é’®ï¼Œå‡†å¤‡å¤„ç†ä¸‹ä¸€è¡Œ")
                        else:
                            LOGGER.info("=" * 80)
                            LOGGER.info("è¿™æ˜¯æœ€åä¸€è¡Œï¼Œä¸éœ€è¦ç‚¹å‡»Add NewæŒ‰é’®")
                            LOGGER.info("=" * 80)
                        
                    except Exception as e:
                        error_msg = f"ç¬¬ {row_num + 1} è¡Œæ•°æ®æäº¤å¤±è´¥: {e}"
                        LOGGER.error(f"âŒ {error_msg}")
                        LOGGER.error(traceback.format_exc())
                        
                        source_lot_value = source_lot if 'source_lot' in locals() and source_lot else 'N/A'
                        self.errors.append({
                            'row_index': idx,
                            'step': 'Spark',
                            'error': str(e),
                            'source_lot': source_lot_value,
                            'timestamp': datetime.now().isoformat()
                        })
                        
                        # ç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                        LOGGER.warning(f"âš ï¸ ç¬¬ {row_num + 1} è¡Œæäº¤å¤±è´¥ï¼Œä½†å°†ç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ...")
                        
                        # å¦‚æœä¸æ˜¯æœ€åä¸€è¡Œï¼Œå°è¯•ç‚¹å‡»Add NewæŒ‰é’®ï¼Œä¸ºä¸‹ä¸€è¡Œåšå‡†å¤‡
                        if row_num < len(mir_df) - 1:
                            try:
                                LOGGER.info("å°è¯•ç‚¹å‡»Add NewæŒ‰é’®ï¼Œä¸ºä¸‹ä¸€è¡Œåšå‡†å¤‡...")
                                time.sleep(2.0)  # ç­‰å¾…é¡µé¢ç¨³å®š
                                if self.spark_submitter._click_add_new_button():
                                    LOGGER.info("âœ… å·²ç‚¹å‡»Add NewæŒ‰é’®ï¼Œå¯ä»¥ç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ")
                                    time.sleep(2.0)
                                else:
                                    LOGGER.warning("âš ï¸ ç‚¹å‡»Add NewæŒ‰é’®å¤±è´¥ï¼Œä¸‹ä¸€è¡Œå¯èƒ½æ— æ³•æ­£å¸¸å¤„ç†")
                            except Exception as e2:
                                LOGGER.warning(f"âš ï¸ å°è¯•ç‚¹å‡»Add NewæŒ‰é’®æ—¶å‡ºé”™: {e2}ï¼Œä½†å°†ç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ")
                
                LOGGER.info("=" * 80)
                LOGGER.info("âœ… æ‰€æœ‰VPOæ•°æ®æäº¤è¯·æ±‚å·²å®Œæˆ")
                LOGGER.info(f"   æ€»è¡Œæ•°: {len(mir_df)}")
                LOGGER.info(f"   æˆåŠŸ: {len([r for r in self.results if r.get('step') == 'Spark' and r.get('status') == 'success'])}")
                LOGGER.info(f"   å¤±è´¥: {len([e for e in self.errors if e.get('step') == 'Spark'])}")
                LOGGER.info("=" * 80)

            # ------------------------------------------------------------------
            # æ‰€æœ‰æäº¤å®Œæˆåï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œä»Dashboardæ”¶é›†VPOå¹¶å†™å›CSV
            # ------------------------------------------------------------------
            try:
                expected_vpo_count = len(mir_df)
                LOGGER.info("å¼€å§‹ä»Spark Dashboardæ”¶é›†VPOç¼–å·ï¼Œç”¨äºå›å†™åˆ°MIRç»“æœCSV...")
                vpo_list = self.spark_submitter.collect_recent_vpos_from_dashboard(expected_count=expected_vpo_count)

                if not vpo_list:
                    LOGGER.warning("æœªèƒ½ä»Dashboardæ”¶é›†åˆ°ä»»ä½•VPOç¼–å·ï¼Œè·³è¿‡ç”ŸæˆåŒ…å«VPOçš„æ–°CSVã€‚")
                    return

                # é¡µé¢ä¸Šé¡ºåºï¼šæœ€æ–°åœ¨å‰ï¼›MIR CSVé¡ºåºï¼šæœ€æ—©åœ¨å‰
                # éœ€è¦å°†åˆ—è¡¨åå‘åæŒ‰é¡ºåºå¯¹åº”åˆ°æ¯ä¸€è¡Œ
                LOGGER.info("å¼€å§‹å°†æ”¶é›†åˆ°çš„VPOç¼–å·ä¸MIRç»“æœæŒ‰é¡ºåºåŒ¹é…...")
                vpo_list_reversed = list(reversed(vpo_list))

                mir_with_vpo = mir_df.copy()
                vpo_col_name = "VPO"
                if vpo_col_name in mir_with_vpo.columns:
                    LOGGER.warning(f"æ£€æµ‹åˆ°MIRç»“æœä¸­å·²å­˜åœ¨åˆ— '{vpo_col_name}'ï¼Œå°†è¦†ç›–è¯¥åˆ—çš„å€¼ã€‚")

                mir_with_vpo[vpo_col_name] = ""

                max_count = min(len(mir_with_vpo), len(vpo_list_reversed))
                for i in range(max_count):
                    mir_with_vpo.at[mir_with_vpo.index[i], vpo_col_name] = vpo_list_reversed[i]
                    LOGGER.info(f"ç¬¬ {i+1} è¡Œ: SourceLot={mir_with_vpo.iloc[i].get('SourceLot', 'N/A')} , VPO={vpo_list_reversed[i]}")

                # ç”Ÿæˆæ–°çš„å¸¦VPOçš„Excelæ–‡ä»¶ï¼ˆä¿å­˜åˆ°MIRç»“æœç›®å½•ï¼‰
                date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = self.work_subdirs['mir'] / f"MIR_Results_with_VPO_{date_str}.xlsx"
                try:
                    mir_with_vpo.to_excel(output_file, index=False, engine='openpyxl')
                    LOGGER.info(f"âœ… å·²ç”ŸæˆåŒ…å«VPOçš„æ–°Excelæ–‡ä»¶: {output_file}")
                except Exception as e:
                    # å¦‚æœExcelä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜ä¸ºCSV
                    LOGGER.warning(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•ä¿å­˜ä¸ºCSVæ ¼å¼...")
                    csv_file = self.work_subdirs['mir'] / f"MIR_Results_with_VPO_{date_str}.csv"
                    mir_with_vpo.to_csv(csv_file, index=False, encoding="utf-8-sig")
                    output_file = csv_file
                    LOGGER.info(f"âœ… å·²ç”ŸæˆåŒ…å«VPOçš„æ–°CSVæ–‡ä»¶: {output_file}")

                LOGGER.info(f"âœ… å·²ç”ŸæˆåŒ…å«VPOçš„æ–°CSVæ–‡ä»¶: {output_file}")
                LOGGER.info(f"   å…±å†™å…¥ {max_count} æ¡VPOè®°å½•ï¼ˆæ€»è¡Œæ•°: {len(mir_with_vpo)}ï¼‰")
            except Exception as e:
                LOGGER.error(f"ä»Dashboardæ”¶é›†VPOå¹¶å†™å›CSVæ—¶å‡ºé”™: {e}")
                LOGGER.error(traceback.format_exc())
                
        except Exception as e:
            raise WorkflowError(f"æäº¤VPOæ•°æ®åˆ°Sparkç½‘é¡µå¤±è´¥: {e}")
    
    def _step_generate_gts_file(self) -> None:
        """æ­¥éª¤4: ç”ŸæˆGTSå¡«å……æ–‡ä»¶"""
        try:
            LOGGER.info("æ­£åœ¨ç”ŸæˆGTSå¡«å……æ–‡ä»¶...")
            
            # å¯¼å…¥GTSå¡«å……æ¨¡å—
            from .gts_excel_filler import fill_gts_template_from_csv
            
            # æŸ¥æ‰¾æœ€æ–°çš„ MIR_Results_with_VPO_* æ–‡ä»¶ï¼ˆæ”¯æŒExcelå’ŒCSVæ ¼å¼ï¼‰
            # ä¼˜å…ˆåœ¨å·¥ä½œç›®å½•ä¸­æŸ¥æ‰¾ï¼Œå¦‚æœæ²¡æœ‰åˆ™åœ¨outputç›®å½•ä¸­æŸ¥æ‰¾ï¼ˆå‘åå…¼å®¹ï¼‰
            mir_results_dir = self.work_subdirs.get('mir', self.work_dir / '01_MIR')
            vpo_excel_files = sorted(mir_results_dir.glob("MIR_Results_with_VPO_*.xlsx"), reverse=True)
            vpo_csv_files = sorted(mir_results_dir.glob("MIR_Results_with_VPO_*.csv"), reverse=True)
            
            # å¦‚æœå·¥ä½œç›®å½•ä¸­æ²¡æœ‰ï¼Œåˆ™åœ¨outputç›®å½•ä¸­æŸ¥æ‰¾
            if not vpo_excel_files and not vpo_csv_files:
                output_dir = self.config.paths.output_dir
                if output_dir.exists():
                    vpo_excel_files = sorted(output_dir.glob("MIR_Results_with_VPO_*.xlsx"), reverse=True)
                    vpo_csv_files = sorted(output_dir.glob("MIR_Results_with_VPO_*.csv"), reverse=True)
            
            if vpo_excel_files:
                vpo_files = vpo_excel_files
            elif vpo_csv_files:
                vpo_files = vpo_csv_files
            else:
                raise WorkflowError("æœªæ‰¾åˆ° MIR_Results_with_VPO_* æ–‡ä»¶ï¼Œè¯·å…ˆå®Œæˆ Spark æ­¥éª¤\næ”¯æŒæ ¼å¼: .xlsx, .csv")
            
            input_file = vpo_files[0]
            LOGGER.info(f"ä½¿ç”¨è¾“å…¥æ–‡ä»¶: {input_file.name}")
            
            # æŸ¥æ‰¾GTSæ¨¡æ¿æ–‡ä»¶
            base_dir = Path(__file__).parent.parent
            template_path = base_dir / "input" / "GTS_Submit.xlsx"
            if not template_path.exists():
                # å°è¯•å…¶ä»–å¯èƒ½çš„ä½ç½®
                possible_paths = [
                    base_dir / "input" / "GTS_Submit.xlsx",
                    self.config.paths.input_dir / "GTS_Submit.xlsx",
                ]
                for path in possible_paths:
                    if path.exists():
                        template_path = path
                        break
                else:
                    raise WorkflowError(f"æœªæ‰¾åˆ°GTSæ¨¡æ¿æ–‡ä»¶ã€‚å·²æ£€æŸ¥: {possible_paths}")
            
            # è°ƒç”¨å¡«å……å‡½æ•°ï¼Œè¾“å‡ºåˆ°GTSæ–‡ä»¶ç›®å½•
            output_file = fill_gts_template_from_csv(
                input_file,
                template_path,
                self.work_subdirs['gts']
            )
            
            if output_file and output_file.exists():
                LOGGER.info(f"âœ… GTSå¡«å……æ–‡ä»¶å·²ç”Ÿæˆ: {output_file.name}")
            else:
                raise WorkflowError("ç”ŸæˆGTSå¡«å……æ–‡ä»¶å¤±è´¥")
                
        except Exception as e:
            raise WorkflowError(f"ç”ŸæˆGTSå¡«å……æ–‡ä»¶å¤±è´¥: {e}")
    
    def _step_submit_to_gts(self) -> None:
        """æ­¥éª¤5: è‡ªåŠ¨å¡«å……å¹¶æäº¤GTS"""
        try:
            LOGGER.info("æ­£åœ¨æ‰“å¼€GTSé¡µé¢å¹¶è‡ªåŠ¨å¡«å……...")
            
            # æ›´æ–°GTS submitterçš„è¾“å‡ºç›®å½•ä¸ºå·¥ä½œç›®å½•çš„GTSæ–‡ä»¶ç›®å½•
            self.gts_submitter.config.output_dir = self.work_subdirs['gts']
            
            # è°ƒç”¨æ–°çš„è‡ªåŠ¨å¡«å……é€»è¾‘
            self.gts_submitter.fill_ticket_with_latest_output()
            
            LOGGER.info("âœ… GTS å¡«å……å’Œæäº¤æµç¨‹å·²å®Œæˆ")
            
        except Exception as e:
            LOGGER.error(f"GTSè‡ªåŠ¨å¡«å……å¤±è´¥: {e}")
            LOGGER.error(traceback.format_exc())
            raise WorkflowError(f"æäº¤GTSå¤±è´¥: {e}")
    
    def _merge_mir_with_spark_config(self, mir_df: pd.DataFrame, spark_config_file: Path | None) -> pd.DataFrame:
        """
        åˆå¹¶ MIR ç»“æœæ–‡ä»¶å’Œ For Spark.csvï¼Œç”Ÿæˆæ±‡æ€»æ–‡ä»¶
        
        Args:
            mir_df: MIRç»“æœDataFrame
            spark_config_file: For Spark.csv æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        merged_df = mir_df.copy()
        
        # é¦–å…ˆå¤„ç† Units_Count_Expected åˆ° Quantity çš„æ˜ å°„ï¼ˆæ— è®ºæ˜¯å¦æœ‰ For Spark.csvï¼‰
        if 'Units_Count_Expected' in merged_df.columns:
            LOGGER.info("å‘ç° Units_Count_Expected åˆ—ï¼Œå°†å…¶æ˜ å°„åˆ° Quantity åˆ—...")
            # å¦‚æœ Quantity åˆ—ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
            if 'Quantity' not in merged_df.columns:
                merged_df['Quantity'] = None
            
            # å°† Units_Count_Expected çš„å€¼å¤åˆ¶åˆ° Quantityï¼ˆå¦‚æœ Quantity ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼‰
            mask_quantity_empty = merged_df['Quantity'].isna() | (merged_df['Quantity'] == '')
            merged_df.loc[mask_quantity_empty, 'Quantity'] = merged_df.loc[mask_quantity_empty, 'Units_Count_Expected']
            LOGGER.info(f"å·²å°† {mask_quantity_empty.sum()} è¡Œçš„ Units_Count_Expected æ˜ å°„åˆ° Quantity")
        elif 'Units_Count_Actual' in merged_df.columns:
            # å¦‚æœæ²¡æœ‰ Units_Count_Expectedï¼Œå°è¯•ä½¿ç”¨ Units_Count_Actual
            LOGGER.info("å‘ç° Units_Count_Actual åˆ—ï¼Œå°†å…¶æ˜ å°„åˆ° Quantity åˆ—...")
            if 'Quantity' not in merged_df.columns:
                merged_df['Quantity'] = None
            mask_quantity_empty = merged_df['Quantity'].isna() | (merged_df['Quantity'] == '')
            merged_df.loc[mask_quantity_empty, 'Quantity'] = merged_df.loc[mask_quantity_empty, 'Units_Count_Actual']
            LOGGER.info(f"å·²å°† {mask_quantity_empty.sum()} è¡Œçš„ Units_Count_Actual æ˜ å°„åˆ° Quantity")
        
        if not spark_config_file or not spark_config_file.exists():
            LOGGER.info("æœªæ‰¾åˆ° For Spark.csv æ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶ï¼ˆä½†å·²å¤„ç† Units_Count_Expected æ˜ å°„ï¼‰")
            return merged_df
        
        try:
            LOGGER.info(f"è¯»å– For Spark.csv æ–‡ä»¶: {spark_config_file}")
            spark_config_df = read_excel_file(spark_config_file)
            
            if spark_config_df.empty:
                LOGGER.warning("For Spark.csv æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡åˆå¹¶")
                return merged_df
            
            # æŸ¥æ‰¾ SourceLot åˆ—ï¼ˆåœ¨ MIR ç»“æœä¸­ï¼‰
            mir_source_lot_col = None
            for col in merged_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT']:
                    mir_source_lot_col = col
                    break
            
            if not mir_source_lot_col:
                LOGGER.warning("MIRç»“æœæ–‡ä»¶ä¸­æœªæ‰¾åˆ° SourceLot åˆ—ï¼Œè·³è¿‡åˆå¹¶")
                return merged_df
            
            # æŸ¥æ‰¾ SourceLot åˆ—ï¼ˆåœ¨ For Spark.csv ä¸­ï¼‰
            spark_source_lot_col = None
            for col in spark_config_df.columns:
                col_upper = str(col).strip().upper()
                if col_upper in ['SOURCELOT', 'SOURCE LOT', 'SOURCE_LOT']:
                    spark_source_lot_col = col
                    break
            
            if not spark_source_lot_col:
                LOGGER.warning("For Spark.csv ä¸­æœªæ‰¾åˆ° SourceLot åˆ—ï¼Œè·³è¿‡åˆå¹¶")
                return merged_df
            
            # æ ‡å‡†åŒ–åˆ—åï¼ˆç»Ÿä¸€ä¸º Source Lotï¼‰
            if mir_source_lot_col != 'Source Lot':
                merged_df = merged_df.rename(columns={mir_source_lot_col: 'Source Lot'})
                mir_source_lot_col = 'Source Lot'
            
            if spark_source_lot_col != 'Source Lot':
                spark_config_df = spark_config_df.rename(columns={spark_source_lot_col: 'Source Lot'})
                spark_source_lot_col = 'Source Lot'
            
            # æ ‡å‡†åŒ– Source Lot å€¼ï¼ˆå»é™¤ç©ºæ ¼ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²ï¼‰
            merged_df['Source Lot'] = merged_df['Source Lot'].astype(str).str.strip()
            spark_config_df['Source Lot'] = spark_config_df['Source Lot'].astype(str).str.strip()
            
            # å»ºç«‹ For Spark.csv çš„æ˜ å°„ï¼ˆä»¥ Source Lot ä¸ºé”®ï¼‰
            spark_config_dict = {}
            for _, row in spark_config_df.iterrows():
                source_lot = str(row['Source Lot']).strip()
                if source_lot and source_lot != 'nan':
                    spark_config_dict[source_lot] = row.to_dict()
            
            LOGGER.info(f"ä» For Spark.csv åŠ è½½äº† {len(spark_config_dict)} ä¸ª Source Lot çš„é…ç½®")
            
            # åˆå¹¶æ•°æ®ï¼šå°† For Spark.csv ä¸­çš„åˆ—æ·»åŠ åˆ° MIR ç»“æœä¸­
            # å¦‚æœ MIR ç»“æœä¸­å·²æœ‰è¯¥åˆ—ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨ For Spark.csv çš„å€¼ï¼ˆå¦‚æœéç©ºï¼‰
            # ç‰¹åˆ«å¤„ç†ï¼šPart Type å­—æ®µå§‹ç»ˆä»¥ For Spark.csv ä¸ºå‡†ï¼ˆå³ä½¿ MIR ç»“æœä¸­æœ‰å€¼ï¼‰
            part_type_cols = ['Part Type', 'PartType', 'PART_TYPE', 'Part_Type']  # å¯èƒ½çš„ Part Type åˆ—å
            
            for source_lot, config_row in spark_config_dict.items():
                # æ‰¾åˆ° MIR ç»“æœä¸­åŒ¹é…çš„è¡Œ
                mask = merged_df['Source Lot'] == source_lot
                if mask.any():
                    # å¯¹äºæ¯ä¸ªé…ç½®åˆ—ï¼Œå¦‚æœ MIR ç»“æœä¸­æ²¡æœ‰æˆ–ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ For Spark.csv çš„å€¼
                    for col, value in config_row.items():
                        if col == 'Source Lot':
                            continue
                        if pd.notna(value) and str(value).strip():
                            # å¦‚æœåˆ—ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»º
                            if col not in merged_df.columns:
                                merged_df[col] = None
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ Part Type ç›¸å…³çš„åˆ—
                            col_upper = str(col).strip().upper()
                            is_part_type = any(pt_col.upper() == col_upper for pt_col in part_type_cols)
                            
                            if is_part_type:
                                # Part Type å­—æ®µï¼šå§‹ç»ˆä»¥ For Spark.csv ä¸ºå‡†ï¼Œç›´æ¥è¦†ç›–
                                merged_df.loc[mask, col] = value
                                LOGGER.debug(f"Source Lot '{source_lot}': Part Type ä»¥ For Spark.csv ä¸ºå‡†ï¼Œå€¼='{value}'")
                            else:
                                # å…¶ä»–å­—æ®µï¼šå¦‚æœ MIR ç»“æœä¸­è¯¥åˆ—ä¸ºç©ºï¼Œåˆ™å¡«å……ï¼›å¦åˆ™ç›´æ¥è¦†ç›–ï¼ˆFor Spark.csv ä¼˜å…ˆï¼‰
                                merged_df.loc[mask, col] = merged_df.loc[mask, col].fillna(value)
                                merged_df.loc[mask, col] = value
            
            # æ³¨æ„ï¼šUnits_Count_Expected åˆ° Quantity çš„æ˜ å°„å·²ç»åœ¨å‡½æ•°å¼€å¤´å¤„ç†äº†
            
            LOGGER.info(f"âœ… åˆå¹¶å®Œæˆï¼šMIRç»“æœ ({len(merged_df)} è¡Œ) + For Spark.csv ({len(spark_config_dict)} ä¸ªé…ç½®)")
            LOGGER.info(f"   åˆå¹¶åçš„åˆ—: {merged_df.columns.tolist()}")
            
        except Exception as e:
            LOGGER.warning(f"åˆå¹¶ For Spark.csv æ—¶å‡ºé”™: {e}ï¼Œå°†ä½¿ç”¨åŸå§‹ MIR ç»“æœ")
            import traceback
            LOGGER.debug(traceback.format_exc())
        
        return merged_df
    
    def _step_save_results(self, df: pd.DataFrame) -> Path:
        """ä¿å­˜å¤„ç†ç»“æœ"""
        try:
            # æ·»åŠ å¤„ç†ç»“æœä¿¡æ¯åˆ°æ•°æ®æ¡†
            result_df = df.copy()
            
            # æ·»åŠ å¤„ç†çŠ¶æ€åˆ—
            if self.results:
                results_df = pd.DataFrame(self.results)
                # å¯ä»¥æ ¹æ®éœ€è¦åˆå¹¶ç»“æœä¿¡æ¯åˆ°åŸå§‹æ•°æ®æ¡†
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä¿å­˜åŸå§‹æ•°æ®å’Œå¤„ç†ç»“æœ
            
            # æ·»åŠ é”™è¯¯ä¿¡æ¯åˆ—
            if self.errors:
                errors_df = pd.DataFrame(self.errors)
                # å¯ä»¥å°†é”™è¯¯ä¿¡æ¯åˆå¹¶åˆ°ç»“æœä¸­
            
            # ç”Ÿæˆæ—¥æœŸå­—ç¬¦ä¸²
            date_str = datetime.now().strftime("%Y%m%d")
            
            # ä¿å­˜ç»“æœåˆ°å·¥ä½œç›®å½•
            output_path = save_result_excel(
                result_df,
                self.work_dir,
                date_str
            )
            
            # å¦‚æœå­˜åœ¨é”™è¯¯ï¼Œä¹Ÿä¿å­˜é”™è¯¯æ—¥å¿—åˆ°å·¥ä½œç›®å½•
            if self.errors:
                error_log_path = self.work_dir / f"workflow_errors_{date_str}.csv"
                errors_df = pd.DataFrame(self.errors)
                errors_df.to_csv(error_log_path, index=False, encoding='utf-8-sig')
                LOGGER.warning(f"å­˜åœ¨ {len(self.errors)} ä¸ªé”™è¯¯ï¼Œå·²ä¿å­˜åˆ°: {error_log_path}")
            
            LOGGER.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            return output_path
            
        except Exception as e:
            raise WorkflowError(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

